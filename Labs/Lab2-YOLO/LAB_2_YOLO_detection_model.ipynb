{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fj56WTcr4MJ"
      },
      "source": [
        "#LAB 2: YOLO detection model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jwv-0T0r4MM"
      },
      "source": [
        "<h4><div style=\"text-align: right\"> Due date: 15:00 Oct 28, 2024.  </div> <br>\n",
        "<div style=\"text-align: right\"> Please upload your file and final-report at PLATO before the class in the form of [ID_Name_Lab1.ipynb]. </div></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqQdi09Sr4MN"
      },
      "source": [
        "### *Instructions:*\n",
        "- Write a program implementing a particular algorithm to solve a given problem.   \n",
        "- <span style=\"color:red\">**Report and discuss your results. Analyze the algorithm, theoretically and empirically.**</span>\n",
        "- You must write their own answers and codes (<span style=\"color:red\">**if not you will get a F grade**</span>)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8QjUrTVr4MN"
      },
      "source": [
        "<h2><span style=\"color:blue\">[202055623] [허치영]</span> </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxxagYJDr4MO",
        "outputId": "7024ba3a-7d28-4c86-a17a-1e3c7fcc7d43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This code is written at 2024-10-15 05:50:00.283591\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "print(\"This code is written at \" + str(datetime.datetime.now()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31sI7rxpr4MP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "import os\n",
        "import struct\n",
        "import copy\n",
        "import random\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import math\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import Compose, ToTensor, Resize\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoK5gaoNr4MQ"
      },
      "source": [
        "### 1. What is the Object Detection?\n",
        "\n",
        ">\n",
        "> <img src=\"https://drive.google.com/uc?export=view&id=1pOo5As8Qo8bQOQjOkkpRwjVKcKNB4gSE\" alt=\"no_image\" style=\"width: 800px;\"/>\n",
        ">\n",
        "> - The difference between $\\bf{classification}$ (left) and $\\bf{detection}$ (right) is simple\n",
        ">\n",
        "> - In image classification, the entire image is classified with a single label\n",
        ">\n",
        "> - In object detection, the network localizes (potentially multiple) objects within the image\n",
        ">\n",
        "> - Therefore, given the image, the obejct detection networks ouput a list of bounding box, class label, and **probability/confidence score**\n",
        ">\n",
        "\n",
        "### 2. About YOLO (Main idea)\n",
        "\n",
        ">\n",
        "> <img src=\"https://drive.google.com/uc?export=view&id=1K7tvcGXdRHBc_t5p-m3uyRrCBFbJ9ze5\" alt=\"no_image\" style=\"width: 800px;\"/>\n",
        ">\n",
        "> - YOLO is an algorithm for object detection\n",
        ">\n",
        "> - They reframe object detection as a $\\bf{single\\ regression\\ problem}$, straight from image pixels to bounding box coordinates and class probabilities\n",
        ">\n",
        "> - Using YOLO, you only look once (YOLO) at an image to predict what objects are present and where they are\n",
        ">\n",
        "> - The advantage of YOLO is the speed, which is extremely fast, since the authors reframe detection as a regression problem not using a complex pipeline\n",
        ">\n",
        "> - What are differences between YOLO and previous detectors?? Refer to https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0s8LPuTr4MQ"
      },
      "source": [
        "### 3. Building a YOLO with PyTorch\n",
        "\n",
        "> ### YOLO model:\n",
        "> - 22Convolutional Layers\n",
        ">\n",
        ">     - Note that, first and 20th conv layers have stride 2 for halving output size\n",
        ">\n",
        "> - 4 Max Pooling Layers\n",
        ">\n",
        "> - 2 Fully Connected Layers\n",
        ">\n",
        "> - Leaky ReLu Activation for all layers except for the last layer\n",
        "\n",
        "### 4. Implementing YOLO\n",
        ">\n",
        "> - The input image is divided into an S $\\times$ S grid of cells\n",
        ">\n",
        "> - Each grid cell predicts B (=2) bounding boxes as well as C (=20) class probabilities\n",
        ">\n",
        "> - Each bounding box consists of 5 predictions: x, y, w, h, and confidence\n",
        ">\n",
        "> - The (x,y) coordinates represent the center of the box relative to the bounds of the grid cell\n",
        ">\n",
        "> - The width and height are predicted relative to the whole image\n",
        ">\n",
        "> - If no object exists in that cell, the confidence score is zero.\n",
        ">\n",
        "> - If the cell contains objects, the confidence score is equal to the intersection over union (IOU) between the predicted box and the ground truth (**This is important for building the loss function**)\n",
        ">\n",
        "> - Thus, the output vector is a S $\\times$ S $\\times$ (B $\\times$ 5 + C) (= 7 $\\times$ 7 $\\times$ 30)\n",
        ">\n",
        "> <img src=\"https://drive.google.com/uc?export=view&id=1LdKtTyYjGKKWmu-4nTYBEMI7n4Sk0aeB\" alt=\"no_image\" style=\"width: 600px;\"/>\n",
        ">\n",
        "> - You must study *generate_target* function in detail, it contains the process of generating output vectors\n",
        ">\n",
        "> Step 1: Load the dataset\n",
        ">\n",
        "> Step 2: Build the YOLO network ($\\bf{4 points}$)\n",
        ">>\n",
        ">>  <img src=\"https://drive.google.com/uc?export=view&id=1c4yOWTZacei0_pFoI47wfxVEhM5Tms8X\" alt=\"no_image\" style=\"width: 800px;\"/>\n",
        ">>\n",
        ">> - Following the YOLO model, you build yolo network\n",
        ">> - **Note that our model is different from network of original paper. Please follow above figure.**\n",
        ">> - You must fill out the blank in **self.pre_train_net** and **self.post_net** blocks\n",
        ">> - All layers use the following Leaky ReLU (0.1 slope for negative part (default=0.01))\n",
        ">>\n",
        ">>\n",
        ">> #### **self.pre_train_net**\n",
        ">>\n",
        ">>\n",
        ">>\n",
        ">>| Name       | Filters              | Output Dimension | Padding |\n",
        ">>|------------|----------------------|------------------| ------- |    \n",
        ">>| Conv 1     | 7 x 7 x 64, stride=2 | 224 x 224 x 64   | 3       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 224 x 224 x 64   |         |\n",
        ">>| Max Pool 1 | 2 x 2, stride=2      | 112 x 112 x 64   |         |\n",
        ">>|\n",
        ">>| Conv 2     | 3 x 3 x 192          | 112 x 112 x 192  | 1       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 112 x 112 x 192  |         |\n",
        ">>| Max Pool 2 | 2 x 2, stride=2      | 56 x 56 x 192    |         |\n",
        ">>|\n",
        ">>| Conv 3     | 1 x 1 x 128          | 56 x 56 x 128    | 0       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 56 x 56 x 128    |         |\n",
        ">>| Conv 4     | 3 x 3 x 256          | 56 x 56 x 256    | 1       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 56 x 56 x 256    |         |\n",
        ">>| Conv 5     | 1 x 1 x 256          | 56 x 56 x 256    | 0       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 56 x 56 x 256    |         |\n",
        ">>| Conv 6     | 3 x 3 x 512          | 56 x 56 x 512    | 1       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 56 x 56 x 512    |         |\n",
        ">>| Max Pool 3 | 2 x 2, stride=2      | 28 x 28 x 512    |         |\n",
        ">>|\n",
        ">>| Conv 7     | 1 x 1 x 256          | 28 x 28 x 256    | 0       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 28 x 28 x 256    |         |\n",
        ">>| Conv 8     | 3 x 3 x 512          | 28 x 28 x 512    | 1       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 28 x 28 x 512    |         |\n",
        ">>| Conv 9     | 1 x 1 x 256          | 28 x 28 x 256    | 0       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 28 x 28 x 256    |         |\n",
        ">>| Conv 10    | 3 x 3 x 512          | 28 x 28 x 512    | 1       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 28 x 28 x 512    |         |\n",
        ">>| Conv 11    | 1 x 1 x 256          | 28 x 28 x 256    | 0       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 28 x 28 x 256    |         |\n",
        ">>| Conv 12    | 3 x 3 x 512          | 28 x 28 x 512    | 1       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 28 x 28 x 512    |         |\n",
        ">>| Conv 13    | 1 x 1 x 256          | 28 x 28 x 256    | 0       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 28 x 28 x 256    |         |\n",
        ">>| Conv 14    | 3 x 3 x 512          | 28 x 28 x 512    | 1       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 28 x 28 x 512    |         |\n",
        ">>| Conv 15    | 1 x 1 x 512          | 28 x 28 x 512    | 0       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 28 x 28 x 512    |         |\n",
        ">>| Conv 16    | 3 x 3 x 1024         | 28 x 28 x 1024   | 1       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 28 x 28 x 1024   |         |\n",
        ">>| Max Pool 4 | 2 x 2, stride=2      | 14 x 14 x 1024   |         |\n",
        ">>|\n",
        ">>| Conv 17    | 1 x 1 x 512          | 14 x 14 x 512    | 0       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 14 x 14 x 512    |         |\n",
        ">>| Conv 18    | 3 x 3 x 1024         | 14 x 14 x 1024   | 1       |\n",
        ">>| LeakyReLU  | negative_slop=0.1   | 14 x 14 x 1024   |         |\n",
        ">>\n",
        ">> #### **self.post_net**\n",
        ">>\n",
        ">>| Name      | Filters                | Output Dimension | Padding |\n",
        ">>|-----------|------------------------|------------------| ------- |\n",
        ">>| Conv 19   | 3 x 3 x 1024           | 14 x 14 x 1024   | 1       |\n",
        ">>| LeakyReLU | negative_slop=0.1     | 14 x 14 x 1024   |         |\n",
        ">>| Conv 20   | 3 x 3 x 1024, stride=2 | 7 x 7 x 1024     | 1       |\n",
        ">>| LeakyReLU | negative_slop=0.1     | 7 x 7 x 1024     |         |\n",
        ">>| Conv 21   | 3 x 3 x 1024           | 7 x 7 x 1024     | 1       |\n",
        ">>| LeakyReLU | negative_slop=0.1     | 7 x 7 x 1024     |         |\n",
        ">>| Conv 22   | 3 x 3 x 1024           | 7 x 7 x 1024     | 1       |\n",
        ">>| LeakyReLU | negative_slop=0.1     | 7 x 7 x 1024     |         |\n",
        ">>\n",
        "> Step 3: Loss function ($\\bf{6 points}$)\n",
        ">\n",
        ">> - Fill out the code considering the loss function in YOLO\n",
        ">> - The loss function is composed like below\n",
        ">> <img src=\"https://drive.google.com/uc?export=view&id=1QcAP__J50M379GAul7SlhAzzU551tuDj\" alt=\"no_image\" style=\"width: 600px;\"/>\n",
        ">>\n",
        ">> - $\\mathbb{1}_{obj}$ is defined as follows:\n",
        ">>    - 1, if an object is present in grid cell $i$ and the $j$th bounding box predictor\n",
        ">>    - 0, otherwise\n",
        ">>\n",
        ">> - YOLO predicts multiple bounding boxes per grid cell (in our case, 2 bounding boxes are predicted)\n",
        ">> - In train stage, YOLO requires one bounding box predictor to be responsible for each object\n",
        ">> - The code assigns one predictor to be \"responsible\" for predicting an object where the predictor has the highest IOU with the ground truth\n",
        ">> - The loss function is divided into the three sub-sections\n",
        ">>    1. Classification ( $\\bf{6}$ classes ) ($\\bf{2 points}$)\n",
        ">>       <img src=\"https://drive.google.com/uc?export=view&id=13mnxg_GNtbWnYLpe1P6zopf693zTYe22\" alt=\"no_image\" style=\"width: 300px;\"/>\n",
        ">>\n",
        ">>       - Similar to sum-squared error for classification except for $\\mathbb{1}_{obj}$ term\n",
        ">>\n",
        ">>       - Note that, we do not penalize classification error when no object is present on the cell (by $\\mathbb{1}_{obj}$ term)\n",
        ">>\n",
        ">>    2. No object appears in cell ($\\bf{2 points}$)\n",
        ">>       <img src=\"https://drive.google.com/uc?export=view&id=1R74HzbRLoX2K5zun8XSLAnRQlKq25XFV\" alt=\"no_image\" style=\"width: 300px;\"/>\n",
        ">>\n",
        ">>       - Compute the loss associated with the confidence score for each bounding box predictor\n",
        ">>\n",
        ">>       - $\\mathbb{1}_{noobj}$ is equal to one when there is no object in the cell, and 0 other wise\n",
        ">>    3. Bounding box coordinates (x,y,h,w) regression (4 scalars), and Object confidence ($\\bf{2 points}$)\n",
        ">>       <img src=\"https://drive.google.com/uc?export=view&id=1lXs5lHUiXGxgMRNCiViY6SyRDL52gQsy\" alt=\"no_image\" style=\"width: 600px;\"/>\n",
        ">>\n",
        ">>       - Compute the loss realted to the predicted bounding box position $(x,y)$ and width/height $(w,h)$\n",
        ">>\n",
        ">>       - Note that width/height are adapted square root before computing the loss\n",
        ">>\n",
        ">>       - In order to solve the problem that small deviations in large boxes matter less than in small boxes\n",
        ">>\n",
        ">>       - Compute the loss associated with the confidence score for the cell which is responsible for the object\n",
        ">>\n",
        ">> - Following the paper, $\\lambda_{coord} = 5$ and $\\lambda_{noobj} = 0.5$\n",
        ">>\n",
        "> Step 4: Train Model ($\\bf{2 points}$)\n",
        ">>\n",
        ">> - Based on the above code, you build train code on your own\n",
        ">> - The necessary things are provided (e.g., *optimizer* and *num_epoch*)\n",
        ">> - Note that, we offer the pre-trained model (0.685 mAP) since it requires large training time.\n",
        ">> - If you construct your code correctly, the performance of mAP will be over 0.7 mAP in 10 epochs (about 15 minitues.)\n",
        ">>\n",
        "> Step 5: Inference\n",
        ">>\n",
        ">> - We offer the test code in below\n",
        ">> - You can check the qualitative results using the visualization code\n",
        ">> - The quantitative results will be evaluated by using the mAP code\n",
        ">> - If the performance of mAP is significantly decreased from the pre-trained model (0.685 mAP), your code will be wrong\n",
        ">>\n",
        ">> Step 5-1: Inference (Visualization)\n",
        ">>\n",
        ">> Step 5-2: Inference (mAP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N46Vgtyr4MR"
      },
      "source": [
        "### Step 1. Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv48AgYQr4MS"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "class YoloDataset(data.Dataset):\n",
        "    def __init__(self, root, list_file, train, transform=None):\n",
        "        self.image_size = 448\n",
        "        self.root = root\n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "        self.fnames = []\n",
        "        self.boxes = []\n",
        "        self.labels = []\n",
        "        self.color_jitter = Compose([transforms.ColorJitter(brightness=0.1,contrast=0.1, saturation=0.1, hue=0.1),\n",
        "                                                 transforms.ToTensor()])\n",
        "        self.plain = Compose([transforms.ToTensor()])\n",
        "        self.resize = Compose([transforms.ToPILImage(),\n",
        "                              transforms.Resize((self.image_size, self.image_size)),\n",
        "                              transforms.ToTensor()])\n",
        "\n",
        "        with open(list_file) as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        for line in lines:\n",
        "            splited = line.strip().split()\n",
        "            self.fnames.append(splited[0])\n",
        "            num_boxes = (len(splited) - 1) // 5\n",
        "            box = []\n",
        "            label = []\n",
        "            for i in range(num_boxes):\n",
        "                x_min = float(splited[1+5*i])\n",
        "                y_min = float(splited[2+5*i])\n",
        "                x_max = float(splited[3+5*i])\n",
        "                y_max = float(splited[4+5*i])\n",
        "                c = splited[5+5*i]\n",
        "                box.append([x_min, y_min, x_max, y_max])\n",
        "                label.append(int(c)+1)\n",
        "            self.boxes.append(torch.Tensor(box))\n",
        "            self.labels.append(torch.LongTensor(label))\n",
        "        self.num_samples = len(self.boxes)\n",
        "\n",
        "    def img_pre_process(self, img, boxes, labels):\n",
        "\n",
        "        if random.random() < 0.5:\n",
        "            im_lr = np.fliplr(img).copy()\n",
        "            height, width, c = img.shape\n",
        "            x_min = width - boxes[:, 2]\n",
        "            x_max = width - boxes[:, 0]\n",
        "            boxes[:,0] = x_min\n",
        "            boxes[:,2] = x_max\n",
        "            img = im_lr\n",
        "\n",
        "        ## Converting the image and box information scaling\n",
        "        if random.random() < 0.5:\n",
        "            scale = random.uniform(0.8, 1.2)\n",
        "            height, width, c = img.shape\n",
        "            img = cv2.resize(img, (int(width * scale), height))\n",
        "            boxes = boxes * torch.FloatTensor([[scale, 1, scale, 1]]).expand_as(boxes)\n",
        "\n",
        "        ## Image translation\n",
        "        if random.random() < 0.5:\n",
        "            center = (boxes[:,2:] + boxes[:,:2]) / 2\n",
        "            height, width, c = img.shape\n",
        "            shifted_img = np.zeros((height, width, c), dtype=img.dtype)\n",
        "\n",
        "            w_range = int(random.uniform(-width*0.2, width*0.2))\n",
        "            h_range = int(random.uniform(-height*0.2, height*0.2))\n",
        "\n",
        "            if w_range<0 and h_range<0:\n",
        "                shifted_img[:height+h_range,:width+w_range,:] = img[-h_range:,-w_range:,:]\n",
        "\n",
        "            elif w_range<0 and h_range>=0:\n",
        "                shifted_img[h_range:,:width+w_range,:] = img[:height-h_range,-w_range:,:]\n",
        "\n",
        "            elif w_range>=0 and h_range>=0:\n",
        "                shifted_img[h_range:,w_range:,:] = img[:height-h_range,:width-w_range,:]\n",
        "\n",
        "            elif w_range>=0 and h_range<0:\n",
        "                shifted_img[:height+h_range,w_range:,:] = img[-h_range:,:width-w_range,:]\n",
        "\n",
        "            wh_range = torch.FloatTensor([[w_range, h_range]]).expand_as(center)\n",
        "            center = center + wh_range\n",
        "\n",
        "            mask1 = (center[:,0] > 0) & (center[:,0] < width)\n",
        "            mask2 = (center[:,1] > 0) & (center[:,1] < height)\n",
        "            mask = (mask1 & mask2).view(-1,1)\n",
        "\n",
        "            boxes_in = boxes[mask.expand_as(boxes)].view(-1,4)\n",
        "\n",
        "            if len(boxes_in) != 0:\n",
        "                box_shift = torch.FloatTensor([[w_range, h_range, w_range, h_range]]).expand_as(boxes_in)\n",
        "                boxes_in = boxes_in + box_shift\n",
        "\n",
        "                labels_in = labels[mask.view(-1)]\n",
        "                labels = labels_in\n",
        "                img = shifted_img\n",
        "                boxes = boxes_in\n",
        "\n",
        "        ## Image cropping\n",
        "        if random.random() < 0.5:\n",
        "            center = (boxes[:,2:] + boxes[:,:2]) / 2\n",
        "            height, width, c = img.shape\n",
        "            h = int(random.uniform(0.6*height,height))\n",
        "            w = int(random.uniform(0.6*width,width))\n",
        "            x = int(random.uniform(0,width-w))\n",
        "            y = int(random.uniform(0,height-h))\n",
        "\n",
        "            center = center - torch.FloatTensor([[x, y]]).expand_as(center)\n",
        "            mask1 = (center[:,0]>0) & (center[:,0]<w)\n",
        "            mask2 = (center[:,1]>0) & (center[:,1]<h)\n",
        "            mask = (mask1 & mask2).view(-1, 1)\n",
        "\n",
        "            boxes_in = boxes[mask.expand_as(boxes)].view(-1,4)\n",
        "\n",
        "            if (len(boxes_in)!=0):\n",
        "                box_shift = torch.FloatTensor([x,y,x,y]).expand_as(boxes_in)\n",
        "\n",
        "                boxes_in = boxes_in - box_shift\n",
        "                boxes_in[:,0]=boxes_in[:,0].clamp_(min=0,max=w)\n",
        "                boxes_in[:,2]=boxes_in[:,2].clamp_(min=0,max=w)\n",
        "                boxes_in[:,1]=boxes_in[:,1].clamp_(min=0,max=h)\n",
        "                boxes_in[:,3]=boxes_in[:,3].clamp_(min=0,max=h)\n",
        "\n",
        "                labels = labels[mask.view(-1)]\n",
        "                img_cropped = img[y:y+h, x:x+w, :]\n",
        "                boxes = boxes_in\n",
        "                img = img_cropped\n",
        "\n",
        "        return img, boxes, labels\n",
        "\n",
        "    def generate_target(self, img, boxes, labels):\n",
        "        grid_size = 7\n",
        "        h,w,_ = img.shape\n",
        "        boxes /= torch.Tensor([w,h,w,h]).expand_as(boxes) # Converting the box size 0 ~ 1\n",
        "        target = torch.zeros((grid_size, grid_size, 30))\n",
        "        cell_size = 1./grid_size\n",
        "\n",
        "        wh = boxes[:,2:] - boxes[:,:2]\n",
        "        cxcy = (boxes[:,2:] + boxes[:,:2]) / 2\n",
        "\n",
        "        for i in range(cxcy.size()[0]):\n",
        "            cxcy_sample = cxcy[i] # center pixel\n",
        "            ij = (cxcy_sample/cell_size).ceil()-1\n",
        "            target[int(ij[1]),int(ij[0]),4] = 1\n",
        "            target[int(ij[1]),int(ij[0]),9] = 1\n",
        "            target[int(ij[1]),int(ij[0]),int(labels[i])+9] = 1\n",
        "\n",
        "            xy = ij*cell_size\n",
        "            delta_xy = (cxcy_sample -xy)/cell_size # Converting delta_xy to the size\n",
        "                                                   # relative to the bounds of the grid cell\n",
        "            target[int(ij[1]),int(ij[0]),2:4] = wh[i]\n",
        "            target[int(ij[1]),int(ij[0]),:2] = delta_xy\n",
        "            target[int(ij[1]),int(ij[0]),7:9] = wh[i]\n",
        "            target[int(ij[1]),int(ij[0]),5:7] = delta_xy\n",
        "\n",
        "        return target\n",
        "\n",
        "    def RandomBrightness(self,bgr):\n",
        "        if random.random() < 0.5:\n",
        "            hsv = cv2.cvtColor(bgr,cv2.COLOR_BGR2HSV)\n",
        "            h,s,v = cv2.split(hsv)\n",
        "            adjust = random.choice([0.5,1.5])\n",
        "            v = v*adjust\n",
        "            v = np.clip(v, 0, 255).astype(hsv.dtype)\n",
        "            hsv = cv2.merge((h,s,v))\n",
        "            bgr = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
        "        return bgr\n",
        "    def RandomSaturation(self,bgr):\n",
        "        if random.random() < 0.5:\n",
        "            hsv = cv2.cvtColor(bgr,cv2.COLOR_BGR2HSV)\n",
        "            h,s,v = cv2.split(hsv)\n",
        "            adjust = random.choice([0.5,1.5])\n",
        "            s = s*adjust\n",
        "            s = np.clip(s, 0, 255).astype(hsv.dtype)\n",
        "            hsv = cv2.merge((h,s,v))\n",
        "            bgr = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
        "        return bgr\n",
        "    def RandomHue(self,bgr):\n",
        "        if random.random() < 0.5:\n",
        "            hsv = cv2.cvtColor(bgr,cv2.COLOR_BGR2HSV)\n",
        "            h,s,v = cv2.split(hsv)\n",
        "            adjust = random.choice([0.5,1.5])\n",
        "            h = h*adjust\n",
        "            h = np.clip(h, 0, 255).astype(hsv.dtype)\n",
        "            hsv = cv2.merge((h,s,v))\n",
        "            bgr = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR)\n",
        "        return bgr\n",
        "\n",
        "    def randomBlur(self,bgr):\n",
        "        if random.random()<0.5:\n",
        "            bgr = cv2.blur(bgr,(5,5))\n",
        "        return bgr\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.fnames[idx]\n",
        "        img = cv2.imread(os.path.join(self.root+fname))\n",
        "        boxes = self.boxes[idx].clone()\n",
        "        labels = self.labels[idx].clone()\n",
        "\n",
        "        if self.train:\n",
        "            img = self.randomBlur(img)\n",
        "            img = self.RandomBrightness(img)\n",
        "            img = self.RandomHue(img)\n",
        "            img = self.RandomSaturation(img)\n",
        "            img, boxes, labels = self.img_pre_process(img, boxes, labels)\n",
        "\n",
        "        target = self.generate_target(img, boxes, labels) # 7x7x30\n",
        "\n",
        "        # Image processing\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        img = self.plain(img)\n",
        "        img = self.resize(img)\n",
        "        img = 2 * img - 1\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7SJzpcr4MT"
      },
      "source": [
        "### Step 2. Build the YOLO network (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btFOH7vmr4MT"
      },
      "source": [
        "> You should build model using nn.Sequential. Otherwise, problem arises when loading pretrained weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scwAcbe-r4MT"
      },
      "outputs": [],
      "source": [
        "class YOLO(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(YOLO, self).__init__()\n",
        "\n",
        "        self.pre_train_net = nn.Sequential(\n",
        "            ################\n",
        "            ### Fill out ###\n",
        "            ################\n",
        "\n",
        "        )\n",
        "\n",
        "        self.post_net = nn.Sequential(\n",
        "            ################\n",
        "            ### Fill out ###\n",
        "            ################\n",
        "        )\n",
        "\n",
        "        self.FC1 = nn.Sequential(\n",
        "            nn.Linear(50176, 4096),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout()\n",
        "        )\n",
        "\n",
        "        self.FC2 = nn.Sequential(\n",
        "            nn.Linear(4096, 1470)\n",
        "        )\n",
        "\n",
        "        self.pre_train_net.cuda()\n",
        "        self.post_net.cuda()\n",
        "        self.FC1.cuda()\n",
        "        self.FC2.cuda()\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.pre_train_net(x)\n",
        "        output = self.post_net(output)\n",
        "\n",
        "        # Ready to going for FC layer\n",
        "        output = output.view(output.size(0), -1)\n",
        "\n",
        "        output = self.FC1(output)\n",
        "        output = self.FC2(output)\n",
        "\n",
        "        output = output.view(output.size(0), 7, 7, 30)\n",
        "        # Converting the output shape to the (batch_size, 7, 7, 30)\n",
        "        output = F.relu(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrjVAtV2r4MT"
      },
      "source": [
        "### Step 3. Loss function (6 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yanhgl8Dr4MT"
      },
      "outputs": [],
      "source": [
        "class yoloLoss(nn.Module):\n",
        "    def __init__(self,l_coord,l_noobj):\n",
        "        super(yoloLoss,self).__init__()\n",
        "        self.l_coord = l_coord\n",
        "        self.l_noobj = l_noobj\n",
        "\n",
        "    def compute_prob_error(self, pred, target): ## classification error\n",
        "        contain = target[:,:,:,4] > 0 # [Batch_size, 7, 7, 1]\n",
        "        contain = contain.unsqueeze(3).expand_as(target) # [Batch_size, 7, 7, 30]\n",
        "\n",
        "        ################\n",
        "        ### Fill out ###\n",
        "        ################\n",
        "\n",
        "        return prob_loss\n",
        "\n",
        "    def not_contain_obj_error(self, pred, target): ## Error where no object appears in cell\n",
        "        not_contain = target[:,:,:,4] == 0 # [Batch_size, 7, 7, 1]\n",
        "        not_contain = not_contain.unsqueeze(3).expand_as(target) # [Batch_size, 7, 7, 30]\n",
        "\n",
        "        ################\n",
        "        ### Fill out ###\n",
        "        ################\n",
        "\n",
        "        return noobj_loss1\n",
        "\n",
        "\n",
        "    def contain_obj_error(self, pred, target): ##\n",
        "        contain = target[:,:,:,4] > 0 # [Batch_size, 7, 7, 1]\n",
        "        contain = contain.unsqueeze(3).expand_as(target) # [Batch_size, 7, 7, 30]\n",
        "\n",
        "        contain_pred = pred[contain].contiguous().view(-1,30) # Only cell which contains the object are remained\n",
        "        box_pred = contain_pred[:,:10].contiguous().view(-1,5) # Reshaping\n",
        "\n",
        "        contain_target = target[contain].contiguous().view(-1,30)\n",
        "        box_target = contain_target[:,:10].contiguous().view(-1,5)\n",
        "\n",
        "        # Note that, We assign one predictor to be \"responsible\" for predicting\n",
        "        # an object based on which prediction has the highest current IOU with the ground truth\n",
        "        # Then, other prediction is \"not responsible\" for predicting an object\n",
        "        # That component is included in \"contain_noobj_mask variables\n",
        "\n",
        "        contain_obj_mask = torch.cuda.ByteTensor(box_target.size()).zero_() # Create the mask\n",
        "        contain_noobj_mask = torch.cuda.ByteTensor(box_target.size()).zero_()\n",
        "\n",
        "        target_iou_gt = torch.zeros(box_target[:,0].size()).cuda() # In order to save the IoU for confidence\n",
        "                                                                   # of the cell which contains object\n",
        "\n",
        "        for i in range(0, box_target.size()[0], 2):\n",
        "            box_p = box_pred[i:i+2] # For 2 bounding boxes information\n",
        "            box_p_coord = Variable(torch.FloatTensor(box_p.size())) # Box coordinate (x_min, y_min, x_max, y_max)\n",
        "            box_p_coord[:,:2] = ## Fill out\n",
        "            box_p_coord[:,2:4] = ## Fill out\n",
        "\n",
        "            box_t = box_target[i].contiguous().view(-1,5)\n",
        "            box_t_coord = Variable(torch.FloatTensor(box_t.size()))\n",
        "            box_t_coord[:,:2] = ## Fill out\n",
        "            box_t_coord[:,2:4] = ## Fill out\n",
        "\n",
        "            iou = self.compute_iou(box_p_coord[:,:4], box_t_coord[:,:4]) # [2,]\n",
        "                                                      # compute IoU between prediction and target boxes,\n",
        "\n",
        "            gt_iou, max_idx = iou.max(0)\n",
        "            contain_obj_mask[i+max_idx] = 1\n",
        "            contain_noobj_mask[i+1-max_idx] = 1\n",
        "\n",
        "            target_iou_gt[i+max_idx] = gt_iou.data.cuda()\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        target_iou_gt = target_iou_gt.to(device)\n",
        "\n",
        "        ## For the cell which contains object\n",
        "        contain_box_pred = ## Fill out\n",
        "\n",
        "        ## For the cell which does not contain object\n",
        "        not_contain_box_pred = ## Fill out\n",
        "\n",
        "        ## For target domain\n",
        "        contain_box_target = ## Fill out\n",
        "        not_contain_box_target = ## Fill out\n",
        "        not_contain_box_target[:,4] = 0\n",
        "\n",
        "        ## For the GT IoU\n",
        "        contain_iou_target = ## Fill out\n",
        "\n",
        "        obj_loss = F.mse_loss(contain_box_pred[:,4], contain_iou_target, size_average=False)\n",
        "        xy_loss = F.mse_loss(contain_box_pred[:,:2], contain_box_target[:,:2], size_average=False)\n",
        "        wh_loss = F.mse_loss(torch.sqrt(contain_box_pred[:,2:4]), torch.sqrt(contain_box_target[:,2:4]), size_average=False)\n",
        "        noobj_loss2 = F.mse_loss(not_contain_box_pred[:,4], not_contain_box_target[:,4], size_average=False)\n",
        "\n",
        "        return obj_loss, xy_loss, wh_loss, noobj_loss2\n",
        "\n",
        "    def compute_iou(self, box_p, box_t):\n",
        "\n",
        "        pred_area = (box_p[:,2] - box_p[:,0]) * (box_p[:,3] - box_p[:,1]) # [2,]\n",
        "        target_area = (box_t[:,2] - box_t[:,0]) * (box_t[:,3] - box_t[:,1]) #[1,]\n",
        "\n",
        "        box_p = box_p.unsqueeze(1).expand(2,1,4)\n",
        "        box_t = box_t.unsqueeze(0).expand(2,1,4)\n",
        "\n",
        "        inter_lt = torch.max(box_p[:,:,:2], box_t[:,:,:2]) # [2,1,2]\n",
        "\n",
        "        inter_rb = torch.min(box_p[:,:,2:], box_t[:,:,2:]) # [2,1,2]\n",
        "\n",
        "        wh = inter_rb - inter_lt # [2,1,2]\n",
        "        wh[wh<0] = 0\n",
        "\n",
        "        inter_area = wh[:,:,0] * wh[:,:,1] # [2,1]\n",
        "        pred_area = pred_area.unsqueeze(1).expand_as(inter_area)\n",
        "        target_area = target_area.unsqueeze(0).expand_as(inter_area)\n",
        "\n",
        "        iou = inter_area / (pred_area + target_area - inter_area)\n",
        "\n",
        "        return iou\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,pred,target):\n",
        "\n",
        "        batch_size = pred.size()[0]\n",
        "\n",
        "        noobj_loss1 = self.not_contain_obj_error(pred, target)\n",
        "        prob_loss = self.compute_prob_error(pred, target)\n",
        "        obj_loss, xy_loss, wh_loss, noobj_loss2 = self.contain_obj_error(pred, target)\n",
        "\n",
        "        total_loss = (self.l_coord*(xy_loss + wh_loss) + obj_loss + self.l_noobj*(noobj_loss1 + noobj_loss2) + prob_loss)\n",
        "\n",
        "        return total_loss / batch_size\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-9K1bTsr4MU"
      },
      "source": [
        "### Step 4. Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmdosiV8r4MU"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS84gro6r4MU"
      },
      "outputs": [],
      "source": [
        "file_root = './lab2/all_img/'\n",
        "train_dataset = YoloDataset(root=file_root, list_file='./lab2/utils/Generate_dataset/voc2007+2012.txt', train=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=4)\n",
        "\n",
        "model = YOLO()\n",
        "state_dict = torch.load('./lab2/pre_train.pt')\n",
        "model.load_state_dict(state_dict['state_dict'])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
        "optimizer.load_state_dict(state_dict['optimizer'])\n",
        "\n",
        "print('pre_train_complete')\n",
        "criterion = yoloLoss(l_coord=5,l_noobj=0.5)\n",
        "\n",
        "best = 1e+30\n",
        "num_epochs = 15\n",
        "print(\"training start!\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        pred = model(images)\n",
        "        loss = criterion(pred, target)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print('Epoch [%d/%d], Iter [%d,%d] Loss:%.4f, average_loss: %.4f' % (epoch+1, num_epochs, i+1, len(train_loader), loss.item(), total_loss / (i+1)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUYyvpAbr4MU"
      },
      "source": [
        "### Step 5. Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "874-Rpj9r4MU"
      },
      "source": [
        "### Step 5-1. Inference (Visualizing)\n",
        "- The below code is for visualizing your results.\n",
        "- Check out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt9FdGT1r4MU"
      },
      "outputs": [],
      "source": [
        "## Visualizing the bouding box image\n",
        "from lab2.util import *\n",
        "\n",
        "SMALL_VOC_CLASSES = ('aeroplane', 'bicycle', 'bus', 'car', 'cat', 'dog')\n",
        "\n",
        "Color = [[0, 0, 0],\n",
        "                    [128, 0, 0],\n",
        "                    [0, 128, 0],\n",
        "                    [128, 128, 0],\n",
        "                    [0, 0, 128],\n",
        "                    [128, 0, 128]]\n",
        "\n",
        "def visualize(model, path):\n",
        "    result = []\n",
        "    image = cv2.imread(path)\n",
        "    h, w, _ = image.shape\n",
        "    img = cv2.resize(image, (448, 448))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    transform = transforms.Compose([transforms.ToTensor(),])\n",
        "    img = 2 * transform(img) - 1\n",
        "    img = Variable(img[None,:,:,:],volatile=True)\n",
        "    img = img.cuda()\n",
        "\n",
        "    pred = model(img) #1x7x7x30\n",
        "    pred = pred.cpu()\n",
        "    boxes,cls_indexs,probs =  interpret_target(pred, 0.1)\n",
        "\n",
        "    for i, box in enumerate(boxes):\n",
        "\n",
        "        x1 = max(int(box[0] * w), 0)\n",
        "        x2 = min(int(box[2] * w), w)\n",
        "        y1 = max(int(box[1] * h), 0)\n",
        "        y2 = min(int(box[3] * h), h)\n",
        "        cls_index = cls_indexs[i]\n",
        "        cls_index = int(cls_index)\n",
        "        prob = probs[i]\n",
        "        prob = float(prob)\n",
        "        result.append([(x1, y1), (x2, y2), SMALL_VOC_CLASSES[cls_index], prob])\n",
        "\n",
        "    img = np.zeros((h, w, 3), np.uint8)\n",
        "    img = img + image[:,:,::-1]\n",
        "\n",
        "    for left_up,right_bottom,class_name,prob in result:\n",
        "        color = Color[SMALL_VOC_CLASSES.index(class_name)]\n",
        "        cv2.rectangle(img, left_up, right_bottom, color, 2)\n",
        "        label = class_name+str(round(prob,2))\n",
        "        text_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)\n",
        "        p1 = (left_up[0], left_up[1]- text_size[1])\n",
        "        cv2.rectangle(img, (p1[0] - 2//2, p1[1] - 2 - baseline), (p1[0] + text_size[0], p1[1] + text_size[1]), color, -1)\n",
        "        cv2.putText(img, label, (p1[0], p1[1] + baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,255,255), 1, 8)\n",
        "\n",
        "    plt.imshow(img)\n",
        "\n",
        "model.eval()\n",
        "visualize(model, './lab2/utils/Generate_dataset/images/003033.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuCFI9PWr4MU"
      },
      "source": [
        "### Step 6-2. Inference (mAP)\n",
        "- The below code is for checkout your network performance,\n",
        "- By using this code, your network prediction is saved in Generate_dataset directory\n",
        "- Then, %run ./lab2/utils/Generate_dataset/main.py\n",
        "- After a few seconds, the mAP is printed.\n",
        "- **Note that if your code is correct, the mAP will be over 70.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "way2EoMAr4MU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "SMALL_VOC_CLASSES = ('aeroplane', 'bicycle', 'bus', 'car', 'cat', 'dog')\n",
        "\n",
        "def write(model, path):\n",
        "    result = []\n",
        "    image = cv2.imread(path)\n",
        "    h, w, _ = image.shape\n",
        "    img = cv2.resize(image, (448, 448))\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    transform = transforms.Compose([transforms.ToTensor(),])\n",
        "    img = 2 * transform(img) - 1\n",
        "    img = Variable(img[None,:,:,:],volatile=True)\n",
        "    img = img.cuda()\n",
        "\n",
        "    pred = model(img) #1x7x7x30\n",
        "    pred = pred.cpu()\n",
        "    boxes,cls_indexs,probs =  interpret_target(pred, 0)\n",
        "\n",
        "    txt_file = open('./lab2/utils/Generate_dataset/predicted/'+os.path.basename(path)[:-4]+'.txt', 'w')\n",
        "\n",
        "    for i, box in enumerate(boxes):\n",
        "\n",
        "        x1 = max(int(box[0] * w), 0)\n",
        "        x2 = min(int(box[2] * w), w)\n",
        "        y1 = max(int(box[1] * h), 0)\n",
        "        y2 = min(int(box[3] * h), h)\n",
        "        cls_index = cls_indexs[i]\n",
        "        cls_index = int(cls_index)\n",
        "        prob = probs[i]\n",
        "        prob = float(prob)\n",
        "        txt_file.write(str(SMALL_VOC_CLASSES[cls_index])+' '+'%.6f'%(prob)+' '+str(x1)+' '+str(y1)+' '+str(x2)+' '+str(y2))\n",
        "        txt_file.write('\\n')\n",
        "    txt_file.close()\n",
        "\n",
        "import glob\n",
        "\n",
        "model.eval()\n",
        "img_names = glob.glob('./lab2/utils/Generate_dataset/images/*')\n",
        "\n",
        "for i, file_name in enumerate(img_names):\n",
        "    write(model, file_name)\n",
        "    if i % 500 == 0:\n",
        "        print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf3uagTdr4MU"
      },
      "outputs": [],
      "source": [
        "%run ./lab2/utils/Generate_dataset/main.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM3U-v9Hr4MU"
      },
      "source": [
        "### *References*\n",
        "[1] https://www.pyimagesearch.com/2018/05/14/a-gentle-guide-to-deep-learning-object-detection/\n",
        "\n",
        "[2] https://github.com/Cartucho/mAP"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
