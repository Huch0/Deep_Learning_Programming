{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9lOBfezfPCX"
   },
   "source": [
    "# LAB 9: Stable diffusion customizing_tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93r6Lo4eAGIL"
   },
   "source": [
    "### **Required Package**\n",
    "- accelerate version: 0.26.1\n",
    "- datasets==2.16.1\n",
    "- diffusers version: 0.26.0.dev0\n",
    "- invisible-watermark version: 0.2.0\n",
    "- numpy version: 1.26.3\n",
    "- omegaconf version: 2.3.0\n",
    "- opencv-python version: 4.9.0.80\n",
    "- peft==0.7.1\n",
    "- scipy version: 1.12.0\n",
    "- torch version: 2.1.2\n",
    "- torchvision version: 0.16.2\n",
    "- transformers version: 4.37.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00Zp3YzIAHQQ"
   },
   "source": [
    "### **Reference**\n",
    "- Diffusers Official Document  [homepage](https://huggingface.co/docs/diffusers/main/en/index)  \n",
    "- Datasets Official Document  [homepage](https://huggingface.co/docs/datasets/index)\n",
    "- segmind blog [blog](https://blog.segmind.com/prompt-guide-for-stable-diffusion-xl-crafting-textual-descriptions-for-image-generation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONK_IcOfewkP"
   },
   "source": [
    "### **필요 패키지 import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wC3rlJYZrmGI"
   },
   "outputs": [],
   "source": [
    "! pip install -q -U \\\n",
    "    datasets \\\n",
    "    peft \\\n",
    "    diffusers \\\n",
    "    bing-image-downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ows5Ibj6rmGI"
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from PIL import Image\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from datasets import load_dataset\n",
    "from bing_image_downloader import downloader\n",
    "from peft import LoraConfig\n",
    "from peft.utils import get_peft_model_state_dict\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PretrainedConfig,\n",
    "    BlipProcessor,\n",
    "    BlipForConditionalGeneration,\n",
    ")\n",
    "\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    StableDiffusionXLPipeline,\n",
    "    UNet2DConditionModel,\n",
    "    StableVideoDiffusionPipeline,\n",
    "    StableDiffusionPipeline\n",
    ")\n",
    "from diffusers.loaders import LoraLoaderMixin\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import cast_training_params\n",
    "from diffusers.utils import (\n",
    "    make_image_grid,\n",
    "    convert_state_dict_to_diffusers,\n",
    "    export_to_video,\n",
    "    export_to_gif,\n",
    ")\n",
    "from diffusers.utils.torch_utils import is_compiled_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF8I5k4BrmGJ"
   },
   "source": [
    "# 1. Custom Dataset 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrb2io9XrmGJ"
   },
   "source": [
    "StableDiffusion과 같은 diffusers에서 지원하는 Diffusion 모델들은 Huggingface에서 지원하는 accelerate, datasets, transformers 와 같은 패키지들과 호환되도록 구현되어 있습니다.\n",
    "LoRA 학습을 위해서는 Huggingface에서 지원하는 datasets 패키지를 이용하여 Custom Dataset을 정의해야 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v82YvbWhrmGJ"
   },
   "source": [
    "## 1-1. Folder 구조 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLdDPhqDrmGJ"
   },
   "outputs": [],
   "source": [
    "custom_dataset_folder_name = \"gen_ai_custom_dataset\"\n",
    "if not os.path.exists(custom_dataset_folder_name):\n",
    "    os.makedirs(custom_dataset_folder_name)\n",
    "if not os.path.exists(os.path.join(custom_dataset_folder_name, \"train\")):\n",
    "    os.makedirs(os.path.join(custom_dataset_folder_name, \"train\"))\n",
    "custom_dataset_folder_path = os.path.join(os.getcwd(), custom_dataset_folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKX5DzMJrmGK"
   },
   "source": [
    "폴더의 구조는 다음과 같이 구성되어 있어야 합니다.\n",
    "    \n",
    "    folder/train/dog/golden_retriever.png\n",
    "    folder/train/cat/maine_coon.png\n",
    "    folder/test/dog/german_shepherd.png\n",
    "    folder/test/cat/bengal.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crfkzAaArmGK"
   },
   "source": [
    "## 1-2. Image 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK82sPUlrmGK"
   },
   "source": [
    "LoRA 학습에 가장 중요한 image들을 모아야합니다.  \n",
    "원하는 image들을 쉽게 모으기 위해서는\n",
    "이번 실습에서는 bing 에서 open license image들을 크롤링 하여 사용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCctnsMgrmGK"
   },
   "outputs": [],
   "source": [
    "query = \"8 bit illustration\"\n",
    "downloader.download(\n",
    "    query,\n",
    "    limit=100,\n",
    "    output_dir=custom_dataset_folder_path,\n",
    "    adult_filter_off=True,\n",
    "    force_replace=False,\n",
    "    timeout=60,\n",
    "    verbose=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZkJm1oQrmGK"
   },
   "outputs": [],
   "source": [
    "# 파일 이름 변경\n",
    "for idx, item in enumerate(\n",
    "    os.listdir(\n",
    "        os.path.join(\n",
    "            custom_dataset_folder_path,\n",
    "            query,\n",
    "        )\n",
    "    )\n",
    "):\n",
    "    format = item.split(\".\")[-1]\n",
    "    # idx format 0 -> 00\n",
    "    idx = str(idx).zfill(2)\n",
    "    os.rename(\n",
    "        os.path.join(custom_dataset_folder_path, query, item),\n",
    "        os.path.join(custom_dataset_folder_path, \"train\", f\"{idx}.{format}\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srrYRh40rmGK"
   },
   "source": [
    "# 1-3. Captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JurkZVcbrmGK"
   },
   "source": [
    "LoRA 학습을 위해서는 이미지와 함께 이미지에 대한 설명을 포함하는 caption이 필요합니다. 이를 위해서는 이미지에 대한 caption을 생성해야 합니다. 이번 예제에서는 이미지에 대한 caption을 생성하기 위해서 이미지 캡셔닝 모델 BLIP을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XoFUg3AXrmGK"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\", torch_dtype=torch.float16\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AyMVzLGormGK"
   },
   "outputs": [],
   "source": [
    "test_image = Image.open(next(iter(glob.glob(os.path.join(custom_dataset_folder_path, \"train\", \"87*\"))), None))\n",
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qz4OxUdrmGK"
   },
   "outputs": [],
   "source": [
    "inputs = processor(test_image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "out = model.generate(**inputs, max_new_tokens=50)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZn4JMULrmGK"
   },
   "source": [
    "## 1-5. Metadata.csv 작성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX1dDvd7rmGK"
   },
   "source": [
    "이러한 Caption들을 지정된 format에 맞추어 csv 혹은 json 포맷으로 저장해야합니다.  \n",
    "csv 포맷이라 하면 아래와 같이  \n",
    "\n",
    "    file_name,additional_feature\n",
    "    0001.png,This is a first value of a text feature you added to your images\n",
    "    0002.png,This is a second value of a text feature you added to your images\n",
    "    0003.png,This is a third value of a text feature you added to your images\n",
    "\n",
    "json 포맷이라 하면  \n",
    "\n",
    "    {\"file_name\": \"0001.png\", \"additional_feature\": \"This is a first value of a text feature you added to your images\"}\n",
    "    {\"file_name\": \"0002.png\", \"additional_feature\": \"This is a second value of a text feature you added to your images\"}\n",
    "    {\"file_name\": \"0003.png\", \"additional_feature\": \"This is a third value of a text feature you added to your images\"}\n",
    "\n",
    "와 같이 작성해야합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vRCHC1hrmGK"
   },
   "source": [
    "이를 위해 pandas dataframe을 이용하여 caption을 저장합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WkUiQuYrmGL"
   },
   "outputs": [],
   "source": [
    "metadata = pandas.DataFrame(columns=[\"file_name\", \"caption\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gl9lhquermGL"
   },
   "outputs": [],
   "source": [
    "for idx, item in enumerate(\n",
    "    os.listdir(os.path.join(custom_dataset_folder_path, \"train\"))\n",
    "):\n",
    "    format = item.split(\".\")[-1]\n",
    "    image_path = os.path.join(custom_dataset_folder_path, \"train\", item)\n",
    "    test_image = Image.open(image_path)\n",
    "    inputs = processor(test_image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "    out = model.generate(**inputs, max_new_tokens=50)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    metadata.loc[idx] = [item, caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D92DDwVTrmGL"
   },
   "outputs": [],
   "source": [
    "# sort by Image column\n",
    "metadata = metadata.sort_values(by=[\"file_name\"]).reset_index(drop=True)\n",
    "metadata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uomabNy_rmGL"
   },
   "outputs": [],
   "source": [
    "# save metadata\n",
    "metadata.to_csv(\n",
    "    os.path.join(custom_dataset_folder_path, \"train\", \"metadata.csv\"), index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WM0pE2CfrmGL"
   },
   "source": [
    "최종 폴더구조는 아래와 같이 이루어지게 됩니다\n",
    "<pre><code>\n",
    "├── gen_ai_custom_dataset\n",
    "│   └── train\n",
    "│   │    └── metadata.csv\n",
    "│   │    └── 00.jpg\n",
    "...\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNiJfg_yrmGL"
   },
   "source": [
    "## 1-6. Dataset 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhGyvzPhrmGL"
   },
   "source": [
    "위와 같은 과정이 마무리 된다면, 이제 customdataset을 정의할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEIiT10vrmGL"
   },
   "outputs": [],
   "source": [
    "custom_dataset = load_dataset(\"imagefolder\", data_dir=custom_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u211QoG3rmGL"
   },
   "outputs": [],
   "source": [
    "custom_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGsgp8jGrmGL"
   },
   "source": [
    "## 2. LoRA 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6uLciLSrmGL"
   },
   "source": [
    "LoRA 학습의 경우 앞 6강 실습-1 에서 사용한 코드를 그대로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8z_NHUrwrmGL"
   },
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87eNKbMTrmGL"
   },
   "source": [
    "#### Prompt Tokenization 을 위한 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doDZ6VrWrmGL"
   },
   "outputs": [],
   "source": [
    "def tokenize_prompt(tokenizer, prompt):\n",
    "    text_inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    ")\n",
    "    text_input_ids = text_inputs.input_ids\n",
    "    return text_input_ids\n",
    "\n",
    "\n",
    "# Adapted from pipelines.StableDiffusionXLPipeline.encode_prompt\n",
    "def encode_prompt(text_encoders, tokenizers, prompt, text_input_ids_list=None):\n",
    "    prompt_embeds_list = []\n",
    "\n",
    "    for i, text_encoder in enumerate(text_encoders):\n",
    "        if tokenizers is not None:\n",
    "            tokenizer = tokenizers[i]\n",
    "            text_input_ids = tokenize_prompt(tokenizer, prompt)\n",
    "        else:\n",
    "            assert text_input_ids_list is not None\n",
    "            try:\n",
    "                text_input_ids = text_input_ids_list[i]\n",
    "            except IndexError:\n",
    "                pass\n",
    "            #text_input_ids = text_input_ids_list[i]\n",
    "\n",
    "        prompt_embeds = text_encoder(\n",
    "            text_input_ids.to(text_encoder.device),\n",
    "            output_hidden_states=True,\n",
    "            return_dict=False,\n",
    "    )\n",
    "\n",
    "        # We are only ALWAYS interested in the pooled output of the final text encoder\n",
    "        pooled_prompt_embeds = prompt_embeds[0]\n",
    "        prompt_embeds = prompt_embeds[-1][-2]\n",
    "        bs_embed, seq_len, _ = prompt_embeds.shape\n",
    "        prompt_embeds = prompt_embeds.view(bs_embed, seq_len, -1)\n",
    "        prompt_embeds_list.append(prompt_embeds)\n",
    "\n",
    "    prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)\n",
    "    pooled_prompt_embeds = pooled_prompt_embeds.view(bs_embed, -1)\n",
    "    return prompt_embeds, pooled_prompt_embeds\n",
    "\n",
    "def import_model_class_from_model_name_or_path(\n",
    "    pretrained_model_name_or_path: str, subfolder: str = \"text_encoder\"\n",
    "):\n",
    "    text_encoder_config = PretrainedConfig.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=subfolder\n",
    ")\n",
    "    model_class = text_encoder_config.architectures[0]\n",
    "\n",
    "    if model_class == \"CLIPTextModel\":\n",
    "        from transformers import CLIPTextModel\n",
    "\n",
    "        return CLIPTextModel(text_encoder_config).from_pretrained(\n",
    "            pretrained_model_name_or_path, subfolder=subfolder\n",
    "        )\n",
    "    elif model_class == \"CLIPTextModelWithProjection\":\n",
    "        from transformers import CLIPTextModelWithProjection\n",
    "\n",
    "        return CLIPTextModelWithProjection(text_encoder_config).from_pretrained(\n",
    "            pretrained_model_name_or_path, subfolder=subfolder\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"{model_class} is not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgGHZ0FIrmGL"
   },
   "source": [
    "### StableDiffusion을 구성하는 세부 모델들 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chPP_G5FrmGP"
   },
   "source": [
    "#### Pretrained Model 및 기타 config 선언\n",
    "학습에 사용될 pretrained model은 \"runwayml/stable-diffusion-v1-5\" 을 이용합니다.  \n",
    "빠른 학습을 위해 Floating Point Precision (부동소수점) 은 16 FP16 을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Me5T_NNnrmGP"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME  = \"runwayml/stable-diffusion-v1-5\"\n",
    "VAE_NAME =\"stabilityai/sd-vae-ft-mse\"\n",
    "VARIANT = \"fp16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8l3Pu6jrmGP"
   },
   "source": [
    "#### Pretrained Model 및 기타 config 선언\n",
    "#### **다음은 StableDiffusionXL을 사용하여 이미지를 생성하는 예시입니다.**  \n",
    "학습에 사용될 pretrained model은 \"stabilityai/stable-diffusion-xl-base-1.0\" 을 이용합니다.  \n",
    "\n",
    "빠른 학습을 위해 Floating Point Precision (부동소수점) 은 16 FP16 을 사용합니다.  \n",
    "다만 FP16으로 학습을 진행하는 경우 현재 \"stabilityai/stable-diffusion-xl-base-1.0\"의 VAE에 오류가 있어, VAE는 \"madebyollin/sdxl-vae-fp16-fix\"를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-WSGmj4rmGP"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME  = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "VAE_NAME =\"madebyollin/sdxl-vae-fp16-fix\"\n",
    "VARIANT = \"fp16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igy6gvLJrmGP"
   },
   "source": [
    "#### Tokenizer, Text Encoder 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxp8mniDrmGP"
   },
   "outputs": [],
   "source": [
    "tokenizer_one = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path = MODEL_NAME,\n",
    "        subfolder = \"tokenizer\",\n",
    "        use_fast = False,\n",
    ")\n",
    "text_encoder_one = import_model_class_from_model_name_or_path(\n",
    "        pretrained_model_name_or_path = MODEL_NAME,\n",
    "        subfolder = \"text_encoder\",\n",
    ")\n",
    "\n",
    "tokenizer_two =  None\n",
    "text_encoder_two = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9i__LpgarmGP"
   },
   "outputs": [],
   "source": [
    "# StableDiffusionXL의 경우 두개의 Text Encoder를 사용하여 두개의 Tokenizer를 불러옵니다.\n",
    "tokenizer_two = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path = MODEL_NAME,\n",
    "        subfolder = \"tokenizer_2\",\n",
    "        use_fast = False,\n",
    ")\n",
    "\n",
    "\n",
    "text_encoder_two = import_model_class_from_model_name_or_path(\n",
    "        pretrained_model_name_or_path = MODEL_NAME,\n",
    "        subfolder = \"text_encoder_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ov4afCOQrmGQ"
   },
   "source": [
    "#### Noise Scheduler, VAE, UNET 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTJQ9Z79rmGQ"
   },
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "        pretrained_model_name_or_path = MODEL_NAME,\n",
    "        subfolder = \"scheduler\"\n",
    ")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    pretrained_model_name_or_path = VAE_NAME,\n",
    ")\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "        pretrained_model_name_or_path = MODEL_NAME,\n",
    "        subfolder = \"unet\",\n",
    "        variant = VARIANT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzOHtnOcrmGQ"
   },
   "source": [
    "추가된 adapter LoRA 레이어들만 학습 시키기 때문 나머지 함수들은 requires_grad_(False)로 설정해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWv5wSJZrmGQ"
   },
   "outputs": [],
   "source": [
    "vae.requires_grad_(False)\n",
    "text_encoder_one.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "print(\"VAE, Text Encoder, and UNet are frozen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cCzBHwArmGQ"
   },
   "outputs": [],
   "source": [
    "# StableDiffusionXL의 경우 두개의 Text Encoder를 사용하여 두개의 Tokenizer를 불러옵니다.\n",
    "text_encoder_two.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxKZmwIdrmGQ"
   },
   "source": [
    "### 학습 루프 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhQHtHhcrmGQ"
   },
   "outputs": [],
   "source": [
    "seed = 2015\n",
    "train_batch_size = 2\n",
    "resolution = 1024\n",
    "max_train_steps = 10000\n",
    "checkpointing_steps = 5000\n",
    "learning_rate = 1e-4\n",
    "output_dir = \"./sd_lora\"\n",
    "\n",
    "# Make sure output_dir exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ssp6KVAmrmGQ"
   },
   "outputs": [],
   "source": [
    "logger = get_logger(__name__)\n",
    "\n",
    "def training_function(dataset, unet, vae, text_encoder_one, text_encoder_two=None):\n",
    "\n",
    "    # Init Accelerator\n",
    "    accelerator = Accelerator(mixed_precision=VARIANT)\n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "\n",
    "    # 세부 모델들 GPU로 올리기, 및 datatype 설정\n",
    "    unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    text_encoder_one.to(accelerator.device, dtype=weight_dtype)\n",
    "    if text_encoder_two is not None:\n",
    "        text_encoder_two.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "    # LoRA config 설정\n",
    "    # UNet의 Attention Layer에 LoRA weights를 추가\n",
    "    # Set correct lora layers\n",
    "    unet_lora_config = LoraConfig(\n",
    "        r=4, # Dimension of LoRA update Matrices\n",
    "        lora_alpha=4, # Dimension of LoRA update Matrices\n",
    "        init_lora_weights=\"gaussian\",\n",
    "        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    "    )\n",
    "    unet.add_adapter(unet_lora_config)\n",
    "\n",
    "    # Trainable Parameter (LoRA weights) 만을 FP32로 Upcast\n",
    "    cast_training_params([unet], dtype=torch.float32)\n",
    "\n",
    "    # Accelerator에 wrap된 모델을 unwrap하는 내부함수\n",
    "    def unwrap_model(model):\n",
    "        model = accelerator.unwrap_model(model)\n",
    "        model = model._orig_mod if is_compiled_module(model) else model\n",
    "        return model\n",
    "\n",
    "\n",
    "    # Custom 된  save & loading hook 함수. LoRA Weight만을 저장하고 불러오는 용도로 사용\n",
    "    # 실질적으로 accelrator.save_state() 함수 사용을 위해 필요한 함수\n",
    "    def save_model_hook(models, weights, output_dir):\n",
    "        if accelerator.is_main_process:\n",
    "            # there are only two options here. Either are just the unet attn processor layers\n",
    "            # or there are the unet and text encoder atten layers\n",
    "            unet_lora_layers_to_save = None\n",
    "            text_encoder_one_lora_layers_to_save = None\n",
    "            if text_encoder_two is not None:\n",
    "                text_encoder_two_lora_layers_to_save = None\n",
    "\n",
    "            for model in models:\n",
    "                if isinstance(unwrap_model(model), type(unwrap_model(unet))):\n",
    "                    unet_lora_layers_to_save = convert_state_dict_to_diffusers(\n",
    "                        get_peft_model_state_dict(model)\n",
    "                    )\n",
    "                elif isinstance(\n",
    "                    unwrap_model(model), type(unwrap_model(text_encoder_one))\n",
    "                ):\n",
    "                    text_encoder_one_lora_layers_to_save = (\n",
    "                        convert_state_dict_to_diffusers(\n",
    "                            get_peft_model_state_dict(model)\n",
    "                        )\n",
    "                    )\n",
    "                elif isinstance(\n",
    "                    unwrap_model(model), type(unwrap_model(text_encoder_two))\n",
    "                ):\n",
    "                    text_encoder_two_lora_layers_to_save = (\n",
    "                        convert_state_dict_to_diffusers(\n",
    "                            get_peft_model_state_dict(model)\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"unexpected save model: {model.__class__}\")\n",
    "\n",
    "                # make sure to pop weight so that corresponding model is not saved again\n",
    "                if weights:\n",
    "                    weights.pop()\n",
    "\n",
    "            if text_encoder_two is not None:\n",
    "                StableDiffusionXLPipeline.save_lora_weights(\n",
    "                    output_dir,\n",
    "                    unet_lora_layers=unet_lora_layers_to_save,\n",
    "                    text_encoder_lora_layers=text_encoder_one_lora_layers_to_save,\n",
    "                    text_encoder_2_lora_layers=text_encoder_two_lora_layers_to_save,\n",
    "                )\n",
    "            else:\n",
    "                StableDiffsuionPipeline.save_lora_weights(\n",
    "                    output_dir,\n",
    "                    unet_lora_layers=unet_lora_layers_to_save,\n",
    "                    text_encoder_lora_layers=text_encoder_one_lora_layers_to_save,\n",
    "                )\n",
    "\n",
    "    def load_model_hook(models, input_dir):\n",
    "        unet_ = None\n",
    "        text_encoder_one_ = None\n",
    "        text_encoder_two_ = None\n",
    "\n",
    "        while len(models) > 0:\n",
    "            model = models.pop()\n",
    "\n",
    "            if isinstance(model, type(unwrap_model(unet))):\n",
    "                unet_ = model\n",
    "            elif isinstance(model, type(unwrap_model(text_encoder_one))):\n",
    "                text_encoder_one_ = model\n",
    "            elif isinstance(model, type(unwrap_model(text_encoder_two))):\n",
    "                text_encoder_two_ = model\n",
    "            else:\n",
    "                raise ValueError(f\"unexpected save model: {model.__class__}\")\n",
    "\n",
    "        lora_state_dict, network_alphas = LoraLoaderMixin.lora_state_dict(input_dir)\n",
    "        LoraLoaderMixin.load_lora_into_unet(\n",
    "            lora_state_dict, network_alphas=network_alphas, unet=unet_\n",
    "        )\n",
    "\n",
    "        text_encoder_state_dict = {\n",
    "            k: v for k, v in lora_state_dict.items() if \"text_encoder.\" in k\n",
    "        }\n",
    "        LoraLoaderMixin.load_lora_into_text_encoder(\n",
    "            text_encoder_state_dict,\n",
    "            network_alphas=network_alphas,\n",
    "            text_encoder=text_encoder_one_,\n",
    "        )\n",
    "        if text_encoder_two_ is not None:\n",
    "            text_encoder_2_state_dict = {\n",
    "                k: v for k, v in lora_state_dict.items() if \"text_encoder_2.\" in k\n",
    "            }\n",
    "            LoraLoaderMixin.load_lora_into_text_encoder(\n",
    "                text_encoder_2_state_dict,\n",
    "                network_alphas = network_alphas,\n",
    "                text_encoder = text_encoder_two_,\n",
    "            )\n",
    "\n",
    "    accelerator.register_save_state_pre_hook(save_model_hook)\n",
    "    accelerator.register_load_state_pre_hook(load_model_hook)\n",
    "\n",
    "    # Optimizer 설정\n",
    "    optimizer_class = torch.optim.AdamW\n",
    "    # Optimizer creation\n",
    "    params_to_optimize = list(filter(lambda p: p.requires_grad, unet.parameters()))\n",
    "    optimizer = optimizer_class(\n",
    "        params_to_optimize,\n",
    "        lr=learning_rate*train_batch_size,\n",
    "        betas = (0.9, 0.999),\n",
    "        weight_decay = 1e-2,\n",
    "        eps = 1e-08,\n",
    "    )\n",
    "\n",
    "    # Preprocessing the datasets\n",
    "    # 먼저 데이터셋의 Column 구분 (Image, Caption)\n",
    "    column_names = dataset[\"train\"].column_names\n",
    "    image_column = column_names[0]\n",
    "    caption_column = column_names[1]\n",
    "\n",
    "   # Captipon 들을 두가지 Text Tokenizer로 Tokenize 하는 내부함수 정의\n",
    "    def tokenize_captions(examples, is_train=True,):\n",
    "        captions = []\n",
    "        for caption in examples[caption_column]:\n",
    "            if isinstance(caption, str):\n",
    "                captions.append(caption)\n",
    "            elif isinstance(caption, (list, np.ndarray)):\n",
    "                # take a random caption if there are multiple\n",
    "                captions.append(random.choice(caption) if is_train else caption[0])\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n",
    "                )\n",
    "        tokens_one = tokenize_prompt(tokenizer_one, captions)\n",
    "        if text_encoder_two is not None:\n",
    "            tokens_two = tokenize_prompt(tokenizer_two, captions)\n",
    "        else:\n",
    "            tokens_two = None\n",
    "        return tokens_one, tokens_two\n",
    "\n",
    "    # Image 를 전처리 하는 부분 정의\n",
    "    train_resize = transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "    train_crop = transforms.CenterCrop(resolution)\n",
    "    train_flip = transforms.RandomHorizontalFlip(p=1.0)\n",
    "    train_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Image 와 Caption preprocessing 을 위한 종합 내부함수정의\n",
    "    def preprocess_train(examples):\n",
    "        images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
    "        # image aug\n",
    "        original_sizes = []\n",
    "        all_images = []\n",
    "        crop_top_lefts = []\n",
    "        for image in images:\n",
    "            original_sizes.append((image.height, image.width))\n",
    "            image = train_resize(image)\n",
    "            if random.random() < 0.5:# flip\n",
    "                image = train_flip(image)\n",
    "            y1 = max(0, int(round((image.height - resolution) / 2.0)))\n",
    "            x1 = max(0, int(round((image.width - resolution) / 2.0)))\n",
    "            image = train_crop(image)\n",
    "            crop_top_left = (y1, x1)\n",
    "            crop_top_lefts.append(crop_top_left)\n",
    "            image = train_transforms(image)\n",
    "            all_images.append(image)\n",
    "\n",
    "        examples[\"original_sizes\"] = original_sizes\n",
    "        examples[\"crop_top_lefts\"] = crop_top_lefts\n",
    "        examples[\"pixel_values\"] = all_images\n",
    "        tokens_one, tokens_two = tokenize_captions(examples)\n",
    "        examples[\"input_ids_one\"] = tokens_one\n",
    "        if text_encoder_two is not None:\n",
    "            examples[\"input_ids_two\"] = tokens_two\n",
    "        return examples\n",
    "\n",
    "    # Training Dataset 생성\n",
    "    train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n",
    "\n",
    "    # DataLoader 생성\n",
    "    def collate_fn(examples):\n",
    "        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "        original_sizes = [example[\"original_sizes\"] for example in examples]\n",
    "        crop_top_lefts = [example[\"crop_top_lefts\"] for example in examples]\n",
    "        input_ids_one = torch.stack([example[\"input_ids_one\"] for example in examples])\n",
    "        if text_encoder_two is not None:\n",
    "            input_ids_two = torch.stack([example[\"input_ids_two\"] for example in examples])\n",
    "            return {\n",
    "                \"pixel_values\": pixel_values,\n",
    "                \"input_ids_one\": input_ids_one,\n",
    "                \"input_ids_two\": input_ids_two,\n",
    "                \"original_sizes\": original_sizes,\n",
    "                \"crop_top_lefts\": crop_top_lefts,\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"pixel_values\": pixel_values,\n",
    "                \"input_ids_one\": input_ids_one,\n",
    "                \"original_sizes\": original_sizes,\n",
    "                \"crop_top_lefts\": crop_top_lefts,\n",
    "            }\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle = True,\n",
    "        collate_fn = collate_fn,\n",
    "        batch_size = train_batch_size,\n",
    "        num_workers = 0,\n",
    "    )\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        'cosine',\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps = 500 * 1,\n",
    "        num_training_steps = max_train_steps *1,\n",
    "    )\n",
    "\n",
    "    # Prepare with  accelerator\n",
    "    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(unet, optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "\n",
    "    # Training 시작 전 기본 세팅\n",
    "    num_train_epochs = math.ceil(max_train_steps / len(train_dataset))\n",
    "    total_batch_size = train_batch_size * accelerator.num_processes\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num Epochs = {num_train_epochs}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {train_batch_size}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "    logger.info(f\"  Total optimization steps = {max_train_steps}\")\n",
    "    global_step = 0\n",
    "    first_epoch = 0\n",
    "    initial_global_step = 0\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        range(0, max_train_steps),\n",
    "        initial = initial_global_step,\n",
    "        desc=\"Steps\",\n",
    "        # Only show the progress bar once on each machine.\n",
    "        disable = not accelerator.is_local_main_process,\n",
    "    )\n",
    "\n",
    "    # Training Loop 시작\n",
    "    for epoch in range(first_epoch, num_train_epochs):\n",
    "        unet.train()\n",
    "        train_loss = 0.0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(unet):\n",
    "                # Convert images to latent space\n",
    "                pixel_values = batch[\"pixel_values\"].to(dtype=weight_dtype)\n",
    "                model_input = vae.encode(pixel_values).latent_dist.sample()\n",
    "                model_input = model_input * vae.config.scaling_factor\n",
    "\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn_like(model_input)\n",
    "\n",
    "                bsz = model_input.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(\n",
    "                    0,\n",
    "                    noise_scheduler.config.num_train_timesteps,\n",
    "                    (bsz,),\n",
    "                    device=model_input.device,\n",
    "                )\n",
    "                timesteps = timesteps.long()\n",
    "\n",
    "                # Add noise to the model input according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_model_input = noise_scheduler.add_noise(model_input, noise, timesteps)\n",
    "\n",
    "                # time ids\n",
    "                def compute_time_ids(original_size, crops_coords_top_left):\n",
    "                    # Adapted from pipeline.StableDiffusionXLPipeline._get_add_time_ids\n",
    "                    target_size = (resolution, resolution)\n",
    "                    add_time_ids = list(original_size + crops_coords_top_left + target_size)\n",
    "                    add_time_ids = torch.tensor([add_time_ids])\n",
    "                    add_time_ids = add_time_ids.to(accelerator.device, dtype=weight_dtype)\n",
    "                    return add_time_ids\n",
    "\n",
    "                add_time_ids = torch.cat(\n",
    "                    [\n",
    "                        compute_time_ids(s, c)\n",
    "                        for s, c in zip(\n",
    "                            batch[\"original_sizes\"], batch[\"crop_top_lefts\"]\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                # Predict the noise residual\n",
    "                unet_added_conditions = {\"time_ids\": add_time_ids}\n",
    "\n",
    "                if text_encoder_two is not None:\n",
    "                    text_input_ids_list = [\n",
    "                        batch[\"input_ids_one\"],\n",
    "                        batch[\"input_ids_two\"],\n",
    "                    ]\n",
    "                    prompt_embeds, pooled_prompt_embeds = encode_prompt(\n",
    "                        text_encoders=[text_encoder_one, text_encoder_two],\n",
    "                        tokenizers=None,\n",
    "                        prompt=None,\n",
    "                        text_input_ids_list=text_input_ids_list,\n",
    "                    )\n",
    "                else:\n",
    "                    text_input_ids_list = [batch[\"input_ids_one\"]]\n",
    "\n",
    "                    prompt_embeds, pooled_prompt_embeds = encode_prompt(\n",
    "                        text_encoders=[text_encoder_one],\n",
    "                        tokenizers=None,\n",
    "                        prompt=None,\n",
    "                        text_input_ids_list=text_input_ids_list,\n",
    "                    )\n",
    "                unet_added_conditions.update({\"text_embeds\": pooled_prompt_embeds})\n",
    "                model_pred = unet(\n",
    "                    noisy_model_input,\n",
    "                    timesteps,\n",
    "                    prompt_embeds,\n",
    "                    added_cond_kwargs=unet_added_conditions,\n",
    "                    return_dict=False,\n",
    "                )[0]\n",
    "\n",
    "\n",
    "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                    target = noise\n",
    "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                    target = noise_scheduler.get_velocity(model_input, noise, timesteps)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Unknown prediction type {noise_scheduler.config.prediction_type}\"\n",
    "                    )\n",
    "\n",
    "                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "\n",
    "                # Gather the losses across all processes for logging (if we use distributed training).\n",
    "                avg_loss = accelerator.gather(loss.repeat(train_batch_size)).mean()\n",
    "                train_loss += avg_loss.item()\n",
    "\n",
    "                # Backpropagate\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(params_to_optimize, 1)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "                accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
    "                train_loss = 0.0\n",
    "\n",
    "                if accelerator.is_main_process:\n",
    "                    if global_step % checkpointing_steps == 0:\n",
    "                        save_path = os.path.join(\n",
    "                            output_dir, f\"checkpoint-{global_step}\"\n",
    "                        )\n",
    "                        accelerator.save_state(save_path)\n",
    "                        logger.info(f\"Saved state to {save_path}\")\n",
    "\n",
    "            logs = {\n",
    "                \"step_loss\": loss.detach().item(),\n",
    "                \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "            }\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if global_step >= max_train_steps:\n",
    "                break\n",
    "    # Training 종료\n",
    "    # Save the lora layers\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        unet = unwrap_model(unet)\n",
    "        unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unet))\n",
    "        StableDiffusionXLPipeline.save_lora_weights(\n",
    "            save_directory=output_dir,\n",
    "            unet_lora_layers=unet_lora_state_dict,\n",
    "        )\n",
    "        del unet\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab9m6hYwrmGR"
   },
   "outputs": [],
   "source": [
    "import accelerate\n",
    "\n",
    "accelerate.notebook_launcher(training_function, args=(custom_dataset, unet, vae, text_encoder_one), num_processes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gohzzg_VrmGR"
   },
   "source": [
    "## Load 학습된 LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGH-HjCNrmGR"
   },
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "                                                pretrained_model_name_or_path = \"runwayml/stable-diffusion-v1-5\",\n",
    "                                                torch_dtype = torch.float16,\n",
    "                                                variant = \"fp16\",\n",
    "                                                use_safetensors = True,\n",
    "                                                )\n",
    "pipeline = pipeline.to(device)\n",
    "print(\"pipeline loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdO6TK1wrmGR"
   },
   "outputs": [],
   "source": [
    "pipeline.load_lora_weights(\"./sd_lora_custom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hefVv5WXrmGR"
   },
   "outputs": [],
   "source": [
    "inference_prompt = \"8bit illustration of city view\"\n",
    "images = pipeline(inference_prompt, num_images_per_prompt=3).images\n",
    "make_image_grid(images, rows=1, cols=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMAT61pcrmGS"
   },
   "source": [
    "학습된 LoRA weight 이외에도, Huggingface hub에 올라와 있는 다양한 LoRA 모델들을 불러올 수 있습니다.\n",
    "\n",
    "아래의 예시들은 Stable Diffusion XL 모델을 불러오는 예시입니다.\n",
    "Colab 환경 아래에서는 실행이 불가할 수 있으니 참고 바랍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtxhuQMRrmGS"
   },
   "outputs": [],
   "source": [
    "pipeline_repo_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "                                                pretrained_model_name_or_path = pipeline_repo_id,\n",
    "                                                torch_dtype = torch.float16,\n",
    "                                                variant = \"fp16\",\n",
    "                                                use_safetensors = True,\n",
    "                                                ).to(device)\n",
    "\n",
    "pipeline.unload_lora_weights()\n",
    "pipeline.load_lora_weights(\"nerijs/lego-minifig-xl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yx5BSNeDrmGS"
   },
   "outputs": [],
   "source": [
    "inference_prompt = \"lego minifig of a kpop idol\"\n",
    "images = pipeline(inference_prompt, num_images_per_prompt=3).images\n",
    "make_image_grid(images, rows=1, cols=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnG_BGI1rmGS"
   },
   "outputs": [],
   "source": [
    "pipeline.unload_lora_weights()\n",
    "pipeline.load_lora_weights(\"goofyai/3d_render_style_xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anYfv6_grmGS"
   },
   "outputs": [],
   "source": [
    "inference_prompt = \"3d style of a man with a hat\"\n",
    "images = pipeline(inference_prompt, num_images_per_prompt=3).images\n",
    "make_image_grid(images, rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g71GD0HnrmGS"
   },
   "source": [
    "동시에 여러개의 LoRA 모델 또한 Fuse하여 사용할 수 있습니다.\n",
    "각각의 LoRA weight들을 불러와 adapter_name을 지정한 후 불러올 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_RWvy0IhrmGS"
   },
   "outputs": [],
   "source": [
    "pipeline.unload_lora_weights()\n",
    "pipeline.load_lora_weights(\"TheLastBen/Papercut_SDXL\", adapter_name=\"paper_cut\")\n",
    "pipeline.load_lora_weights(\"nerijs/pixel-art-xl\", adapter_name=\"pixel_art\")\n",
    "print(\"Weights loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMVuSDQPrmGS"
   },
   "outputs": [],
   "source": [
    "pipeline.set_adapters([\"paper_cut\", \"pixel_art\"], adapter_weights=[0.5, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFFts9agrmGS"
   },
   "outputs": [],
   "source": [
    "inference_prompt = \"A cute hedgehog plushie with a brown back and yellow face with black eyes, pixel art, paper cut\"\n",
    "images = pipeline(inference_prompt, num_images_per_prompt=3).images\n",
    "make_image_grid(images, rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gzlLQ8WrmGS"
   },
   "source": [
    "Huggingface 에 올라와있는 다양한 LoRA Weight들은 아래의 링크에서 확인해보실 수 있습니다.  \n",
    "https://huggingface.co/models?other=base_model:stabilityai/stable-diffusion-xl-base-1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jD3O6mLKrmGS"
   },
   "source": [
    "# 2. Prompting Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWE9J3tErmGS"
   },
   "source": [
    "자연스러운 이미지 생성을 위해서는 좋은 모델을 사용하는 것도 중요하지만, 좋은 모델을 사용하더라도 좋은 prompt를 사용하지 않는다면 원하는 이미지를 생성하기 어렵습니다.\n",
    "다음은 prompt를 사용하는 방법과 다른 parameter들을 조정하여 원하는 이미지를 만들 수 있는에 대한 간단한 설명입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4T_KUbXnrmGS"
   },
   "outputs": [],
   "source": [
    "del pipeline\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bw_gFnihrmGS"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "    variant=\"fp16\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "pipeline = pipeline.to(device)\n",
    "print(\"pipeline loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3mMP_OSrmGS"
   },
   "source": [
    "### 좋은 Prompt의 구성 요소\n",
    "\n",
    "Prompt는 인공지능 모델이 원하는 이미지를 생성하도록 유도하는 핵심 문장입니다. **효과적인 Prompt**는 모델이 명확하고 상세하게 원하는 이미지를 이해하도록 도와줍니다. 그렇다면, 어떤 요소들이 좋은 Prompt를 구성하는지 알아보겠습니다.   \n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 주제 (**Subject**)\n",
    "좋은 Prompt의 핵심은 생성할 이미지의 주제, 즉 **Subject**입니다. Subject는 Prompt의 중심이 되는 객체로, 다음 네 가지 주요 요소를 포함해야 합니다.\n",
    "\n",
    "##### a. 대상 (**Character**)\n",
    "- **Character**는 이미지의 주인공이 됩니다. 이는 사람, 동물, 또는 다른 어떤 캐릭터일 수 있습니다.\n",
    "\n",
    "##### b. 행위 (**Action**)\n",
    "- **Action**은 Character가 수행하는 동작이나 활동을 나타냅니다. 이는 이미지에 동적인 요소를 더해줍니다.\n",
    "\n",
    "##### c. 장소 (**Location**)\n",
    "- **Location**은 Action이 일어나는 배경이 됩니다. 이는 이미지에 공간적 맥락을 제공합니다.\n",
    "\n",
    "##### d. 감정 (**emotion**)\n",
    "- **emotion**은 Character의 감정을 나타냅니다. 이는 이미지에 감정적인 요소를 더해줍니다.\n",
    "\n",
    "Prompt 작성 시 이러한 요소들을 구체적으로 명시함으로써, AI가 보다 정확하고 생생한 이미지를 생성할 수 있도록 유도할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9beQvlvrmGS"
   },
   "outputs": [],
   "source": [
    "# prompt that only contain simple text\n",
    "simple_prompt = \"A man with a hat\"\n",
    "simple_image = pipeline(simple_prompt, num_images_per_prompt=1).images[0]\n",
    "\n",
    "# prompt that contain character, action, location, and emotion\n",
    "advanced_prompt_1 = (\n",
    "    \"A man with a hat sitting on a bench in a bustling city park, looking contemplative\"\n",
    ")\n",
    "advanced_image_1 = pipeline(advanced_prompt_1, num_images_per_prompt=1).images[0]\n",
    "\n",
    "make_image_grid([simple_image, advanced_image_1], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RltJfBUqrmGS"
   },
   "source": [
    "#### 2. Depth, Nuance\n",
    "주제에 대한 설명을 추가했으면 이미지에 깊이와 풍부함을 더하는 구체적인 사항들을 추가해야합니다.\n",
    "\n",
    "##### a. 색상 (**Color**)\n",
    "- **Color**는 이미지의 색상을 나타냅니다. 이는 이미지에 시각적인 요소를 더해줍니다. 생동감 있는 이미지 혹은 파스텔 톤?, 다양한 색상의 이미지를 생성하고 싶다면 이를 추가해보세요.\n",
    "\n",
    "##### b. 질감 (**Texture**)\n",
    "- **Texture**는 이미지의 질감을 나타냅니다. 비단의 부드러움, 나무의 거침, 또는 물체의 광택과 같은 질감을 추가할 수 있습니다.\n",
    "\n",
    "##### c. 상대적 크기 (**Proportions**)\n",
    "- **Proportions**는 이미지 내의 물체들의 상대적인 크기를 나타냅니다. 이는 이미지에 공간적 맥락을 제공하여 조화와 균형을 더해줍니다.\n",
    "\n",
    "##### d. 관점 (**Perspective**)\n",
    "- **Perspective**는 장면이 펼쳐지는 방식을 결정하는 관점으로, 위에서 내려다보는이 될 수도 있고, 옆에서 몰래 보는 관점이 될 수도 있습니다.\n",
    "\n",
    "##### e. 빛과 그림자 (**Reflections and Shadows**)\n",
    "- **Reflections and Shadows**는 빛과 그림자를 나타냅니다. 이는 이미지의 현실감과 깊이, 차원을 더해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvJ1NzlwrmGT"
   },
   "outputs": [],
   "source": [
    "# prompt that contain Color, Texture, proportions, perspective, and lightings\n",
    "advanced_prompt_2 = \"A man with a hat sitting on a bench in a bustling city park, looking contemplative, pastel tone color, rough texture, close up, low angle, low contrast\"\n",
    "advanced_image_2 = pipeline(advanced_prompt_2, num_images_per_prompt=1).images[0]\n",
    "\n",
    "make_image_grid([simple_image, advanced_image_1, advanced_image_2], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCcgxTwDrmGT"
   },
   "source": [
    "#### 3. Artistic Style\n",
    "그림의 스타일 추가하는 요소를 포함하면 그림을 더 완성도 있게 만들 수 있습니다.\n",
    "***Artistic Style*** 이라 하면 다음과 같을 수 있습니다.\n",
    "- Comic Book, Watercolor, Oil Painting, Pencil Sketch, Pointillism, Impressionism, Cubism, Surrealism, Expressionism, Pop Art, Minimalism, Abstract Art, Realism, Hyperrealism, Photorealism, Surrealism, Expressionism, Pop Art, Minimalism, Abstract Art, Realism, Hyperrealism, Photorealism ...\n",
    "\n",
    "SDXL 같은 경우 두개의 Text Encoder를 사용할 수 있으므로, 마지막 요소는 두번째 Text Encoder를 이용하여 추가하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OW6Q_HbJrmGT"
   },
   "outputs": [],
   "source": [
    "advanced_prompt_3 = \"Oil Painting, renaissance style, 8K, finegrained\"\n",
    "advanced_image_3 = pipeline(\n",
    "    prompt=advanced_prompt_2, prompt_2=advanced_prompt_3, num_images_per_prompt=1\n",
    ").images[0]\n",
    "make_image_grid(\n",
    "    [simple_image, advanced_image_1, advanced_image_2, advanced_image_3], rows=2, cols=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sy2DxXYhrmGT"
   },
   "source": [
    "#### 4. Negative Prompt  \n",
    "\n",
    "Stable Diffusion에서 \"Negative Prompt\"는 이미지 생성 과정에서 특정 요소나 특징을 제외시키기 위해 사용되는 기술입니다. 예를 들어, Stable Diffusion을 사용하여 이미지를 생성할 때, 사용자는 원하는 장면이나 객체에 대한 설명을 입력합니다. 그러나 때로는 특정 요소나 스타일을 제외하고 싶을 수 있습니다. 이때 Negative Prompt를 사용합니다.  \n",
    "\n",
    "Negative Prompt는 기본적으로 이미지 생성 모델에게 \"이것을 포함하지 말라\"고 지시하는 역할을 합니다. 예를 들어, \"바다 없이 해변의 일몰\"이라는 이미지를 만들고 싶다면, \"일몰\"이라는 긍정적인 프롬프트와 함께 \"바다 없음\"이라는 부정적인 프롬프트를 사용할 수 있습니다. 이렇게 하면 모델은 해변의 일몰 이미지를 생성하면서 바다 요소를 배제하려고 합니다.  \n",
    "\n",
    "Negative Prompt는 이미지 생성 과정에서 더욱 정밀한 제어를 가능하게 하며, 사용자가 원치 않는 요소를 효과적으로 제거할 수 있도록 도와줍니다. 이는 특히 복잡하거나 상세한 이미지를 생성할 때 유용합니다.  \n",
    "\n",
    "Negative Prompt 또한 일반 prompt와 동일하게 두 개의 Text Encoder를 사용하여 추가할 수 있습니다.  \n",
    "\n",
    "Negative Prompt의 얘로는 아래와 같은 것들이 있습니다.  \n",
    "[ out of frame, lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, username, watermark, signature. ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCuUVp2grmGT"
   },
   "outputs": [],
   "source": [
    "## Negative Promp\n",
    "negative_prompt_1 = \"worst quality, low quality, jpeg artifacts, blurry\"\n",
    "negative_prompt_2 = \"gross proportions, malformed limbs, watermark, signature\"\n",
    "advanced_image_4 = pipeline(\n",
    "    prompt=advanced_prompt_2,\n",
    "    prompt_2=advanced_prompt_3,\n",
    "    negative_prompt=negative_prompt_1,\n",
    "    negative_prompt_2=negative_prompt_2,\n",
    "    num_images_per_prompt=1,\n",
    ").images[0]\n",
    "make_image_grid(\n",
    "    [\n",
    "        advanced_image_1,\n",
    "        advanced_image_2,\n",
    "        advanced_image_3,\n",
    "        advanced_image_4,\n",
    "    ],\n",
    "    rows=2,\n",
    "    cols=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2eMu7m3rmGT"
   },
   "source": [
    "## Guidance Scale\n",
    "Prompt의 영향력은 Guidance Scale 파라미터에 의해 조절됩니다.\n",
    "Guidance Scale의 default value는 5로, 값이 클수록 prompt의 영향력이 커지고, 값이 작을수록 prompt의 영향력이 작아집니다.\n",
    "아래의 예시를 통해 Guidance Scale의 영향력을 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSPiJPAGrmGT"
   },
   "outputs": [],
   "source": [
    "## Guidance Scale\n",
    "advanced_image_5 = pipeline(\n",
    "    prompt=advanced_prompt_2,\n",
    "    prompt_2=advanced_prompt_3,\n",
    "    negative_prompt=negative_prompt_1,\n",
    "    negative_prompt_2=negative_prompt_2,\n",
    "    guidance_scale=0.5,\n",
    "    num_images_per_prompt=1,\n",
    ").images[0]\n",
    "advanced_image_6 = pipeline(\n",
    "    prompt=advanced_prompt_2,\n",
    "    prompt_2=advanced_prompt_3,\n",
    "    negative_prompt=negative_prompt_1,\n",
    "    negative_prompt_2=negative_prompt_2,\n",
    "    guidance_scale=11,\n",
    "    num_images_per_prompt=1,\n",
    ").images[0]\n",
    "make_image_grid(\n",
    "    [\n",
    "        advanced_image_1,\n",
    "        advanced_image_2,\n",
    "        advanced_image_3,\n",
    "        advanced_image_4,\n",
    "        advanced_image_5,\n",
    "        advanced_image_6,\n",
    "    ],\n",
    "    rows=3,\n",
    "    cols=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oo2CW7evrmGT"
   },
   "source": [
    "## Step\n",
    "num_inference_steps 파라미터는 이미지를 생성하는데 필요한 step의 수를 의미합니다.  \n",
    "default 값은 50으로 50 step만큼의 과정을 거쳐 이미지를 생성합니다.  \n",
    "만약 num_inference_steps 값을 줄이면, 더 빠른 속도로 이미지를 생성하지만, 더 낮은 퀄리티의 이미지를 생성하게 됩니다.  \n",
    "반대로 num_inference_steps 값을 늘리면, 더 높은 퀄리티의 이미지를 생성하지만, 더 많은 시간이 소요됩니다.  \n",
    "이를 잘 조절하여 원하는 이미지를 생성해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkYUGZz1rmGT"
   },
   "outputs": [],
   "source": [
    "## Guidance Scale\n",
    "advanced_image_7 = pipeline(\n",
    "    prompt=advanced_prompt_2,\n",
    "    prompt_2=advanced_prompt_3,\n",
    "    negative_prompt=negative_prompt_1,\n",
    "    negative_prompt_2=negative_prompt_2,\n",
    "    guidance_scale=5,\n",
    "    num_inference_steps=10,\n",
    "    num_images_per_prompt=1,\n",
    ").images[0]\n",
    "advanced_image_8 = pipeline(\n",
    "    prompt=advanced_prompt_2,\n",
    "    prompt_2=advanced_prompt_3,\n",
    "    negative_prompt=negative_prompt_1,\n",
    "    negative_prompt_2=negative_prompt_2,\n",
    "    guidance_scale=5,\n",
    "    num_inference_steps=100,\n",
    "    num_images_per_prompt=1,\n",
    ").images[0]\n",
    "\n",
    "make_image_grid(\n",
    "    [\n",
    "        advanced_image_1,\n",
    "        advanced_image_2,\n",
    "        advanced_image_3,\n",
    "        advanced_image_4,\n",
    "        advanced_image_5,\n",
    "        advanced_image_6,\n",
    "        advanced_image_7,\n",
    "        advanced_image_8,\n",
    "    ],\n",
    "    rows=4,\n",
    "    cols=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkzcOg74rmGT"
   },
   "source": [
    "## 3. Stable  Video Diffusion\n",
    "<img src = \"https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt/resolve/main/output_tile.gif\" >\n",
    "\n",
    "Stable Video Diffusion은 Single Still Image를 input으로 받아 중간 frame들을 생성해 short video를 만들어줍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJQBnrYjrmGT"
   },
   "outputs": [],
   "source": [
    "del pipeline\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNH62rE7rmGU"
   },
   "outputs": [],
   "source": [
    "pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-video-diffusion-img2vid-xt\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\",\n",
    ")\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-C5AwLNKrmGU"
   },
   "outputs": [],
   "source": [
    "frames = pipe(advanced_image_3, decode_chunk_size=15).frames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQsLFaFNrmGU"
   },
   "outputs": [],
   "source": [
    "export_to_gif(frames, \"test.gif\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
