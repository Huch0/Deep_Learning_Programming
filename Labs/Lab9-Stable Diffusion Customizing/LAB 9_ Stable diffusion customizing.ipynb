{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9lOBfezfPCX"
   },
   "source": [
    "#LAB 9: Stable diffusion customizing\n",
    "\n",
    "<h4><div style=\"text-align: right\"> Due date: 15:00 Dec 23, 2024.  </div> <br>\n",
    "<div style=\"text-align: right\"> Please upload your file and final-report at PLATO before the class in the form of [ID_Name_Lab9.ipynb]. </div></h4>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Esf3mnLgKTTD"
   },
   "source": [
    "### *Instructions:*\n",
    "- Write a program implementing a particular algorithm to solve a given problem.   \n",
    "- <span style=\"color:red\">**Report and discuss your results. Analyze the algorithm, theoretically and empirically.**</span>\n",
    "- You must write their own answers and codes (<span style=\"color:red\">**if not you will get a F grade**</span>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckkK-rl07OBI"
   },
   "source": [
    "<h2><span style=\"color:blue\">[2020556523] [허치영]</span> </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYv-5OaP7RwD"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(\"This code is written at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93r6Lo4eAGIL"
   },
   "source": [
    "### **Required Package**:\n",
    "- accelerate version: 0.26.1\n",
    "- datasets==2.16.1\n",
    "- diffusers version: 0.26.0.dev0\n",
    "- invisible-watermark version: 0.2.0\n",
    "- numpy version: 1.26.3\n",
    "- omegaconf version: 2.3.0\n",
    "- opencv-python version: 4.9.0.80\n",
    "- peft==0.7.1\n",
    "- scipy version: 1.12.0\n",
    "- torch version: 2.1.2\n",
    "- torchvision version: 0.16.2\n",
    "- transformers version: 4.37.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00Zp3YzIAHQQ"
   },
   "source": [
    "### **Reference**\n",
    "- Diffusers Official Document  [homepage](https://huggingface.co/docs/diffusers/main/en/index)  \n",
    "- Datasets Official Document  [homepage](https://huggingface.co/docs/datasets/index)\n",
    "- segmind blog [blog](https://blog.segmind.com/prompt-guide-for-stable-diffusion-xl-crafting-textual-descriptions-for-image-generation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONK_IcOfewkP"
   },
   "source": [
    "### **필요 패키지 import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3dtxPywKTTG"
   },
   "outputs": [],
   "source": [
    "! pip install -q -U \\\n",
    "    datasets \\\n",
    "    peft \\\n",
    "    diffusers \\\n",
    "    bing-image-downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZ5yeVKyKTTH"
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from PIL import Image\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from datasets import load_dataset\n",
    "from bing_image_downloader import downloader\n",
    "from peft import LoraConfig\n",
    "from peft.utils import get_peft_model_state_dict\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    PretrainedConfig,\n",
    ")\n",
    "\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    StableDiffusionXLPipeline,\n",
    "    UNet2DConditionModel,\n",
    "    StableVideoDiffusionPipeline,\n",
    "    StableDiffusionPipeline\n",
    ")\n",
    "from diffusers.loaders import LoraLoaderMixin\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.training_utils import cast_training_params\n",
    "from diffusers.utils import (\n",
    "    make_image_grid,\n",
    "    convert_state_dict_to_diffusers,\n",
    "    export_to_video,\n",
    "    export_to_gif,\n",
    ")\n",
    "from diffusers.utils.torch_utils import is_compiled_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StGo0AEWKTTI"
   },
   "source": [
    "# 1. Custom Dataset 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfaV32QLKTTI"
   },
   "source": [
    "StableDiffusion과 같은 diffusers에서 지원하는 Diffusion 모델들은 Huggingface에서 지원하는 accelerate, datasets, transformers 와 같은 패키지들과 호환되도록 구현되어 있습니다.\n",
    "LoRA 학습을 위해서는 Huggingface에서 지원하는 datasets 패키지를 이용하여 Custom Dataset을 정의해야 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45HpTJxQKTTJ"
   },
   "source": [
    "## 1-1. Folder 구조 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTIRF6csKTTJ"
   },
   "outputs": [],
   "source": [
    "custom_dataset_folder_name = \"gen_ai_custom_dataset\"\n",
    "if not os.path.exists(custom_dataset_folder_name):\n",
    "    os.makedirs(custom_dataset_folder_name)\n",
    "if not os.path.exists(os.path.join(custom_dataset_folder_name, \"train\")):\n",
    "    os.makedirs(os.path.join(custom_dataset_folder_name, \"train\"))\n",
    "custom_dataset_folder_path = os.path.join(os.getcwd(), custom_dataset_folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkDiBPNAKTTJ"
   },
   "source": [
    "폴더의 구조는 다음과 같이 구성되어 있어야 합니다.\n",
    "    \n",
    "    folder/train/dog/golden_retriever.png\n",
    "    folder/train/cat/maine_coon.png\n",
    "    folder/test/dog/german_shepherd.png\n",
    "    folder/test/cat/bengal.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HieeacPxKTTJ"
   },
   "source": [
    "## 1-2. Image 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NboA96QOKTTK"
   },
   "source": [
    "LoRA 학습에 가장 중요한 image들을 모아야합니다.  \n",
    "원하는 image들을 쉽게 모으기 위해서는\n",
    "이번 실습에서는 bing 에서 open license image들을 크롤링 하여 사용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMR5r0TvKTTK"
   },
   "outputs": [],
   "source": [
    "query = \"8 bit illustration\"\n",
    "downloader.download(\n",
    "    query,\n",
    "    limit=100,\n",
    "    output_dir=custom_dataset_folder_path,\n",
    "    adult_filter_off=True,\n",
    "    force_replace=False,\n",
    "    timeout=60,\n",
    "    verbose=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqWvln-4KTTL"
   },
   "outputs": [],
   "source": [
    "# 파일 이름 변경\n",
    "for idx, item in enumerate(\n",
    "    os.listdir(\n",
    "        os.path.join(\n",
    "            custom_dataset_folder_path,\n",
    "            query,\n",
    "        )\n",
    "    )\n",
    "):\n",
    "    format = item.split(\".\")[-1]\n",
    "    # idx format 0 -> 00\n",
    "    idx = str(idx).zfill(2)\n",
    "    os.rename(\n",
    "        os.path.join(custom_dataset_folder_path, query, item),\n",
    "        os.path.join(custom_dataset_folder_path, \"train\", f\"{idx}.{format}\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qDyAfXMKTTL"
   },
   "source": [
    "# 1-3. Captioning [과제]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9itrdiNhKTTL"
   },
   "source": [
    "LoRA 학습을 위해서는 이미지와 함께 이미지에 대한 설명을 포함하는 caption이 필요합니다. 이를 위해서는 이미지에 대한 caption을 생성해야 합니다. 이미지에 대한 caption을 생성하기 위해서 이미지 캡셔닝 모델 BLIP을 사용하여 Caption을 생성해보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ah8jJhx4KTTL"
   },
   "outputs": [],
   "source": [
    "# Device 정의\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "# 모델은 \"Salesforce/blip-image-captioning-base\"로 processor는 \"Salesforce/blip-image-captioning-large\"로 초기화\n",
    "processor = None  # TODO : BlipProcessor 함수를 사용해서 processor를 초기화 해주세요.\n",
    "model = None  # TODO : BlipForConditionalGeneration 함수를 사용해서 model을 초기화 해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLVwgd_pKTTL"
   },
   "source": [
    "모델 정의가 완료되었다면, 예시 이미지로 캡션을 생성해 보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZUlUgZWKTTL"
   },
   "outputs": [],
   "source": [
    "test_image = None  # TODO : PIL의 Image, glob, os를 활용하여 test 이미지를 불러와주세요.\n",
    "\n",
    "# Processor를 사용하여 이미지를 모델 입력으로 변환\n",
    "inputs =  None  # TODO : processor 함수를 사용해서 이미지를 모델 입력으로 변환해주세요!\n",
    "\n",
    "# Model을 사용하여 이미지 캡션 생성\n",
    "outputs = None  # TODO : model 함수를 사용하여 이미지 캡션을 생성해주세요!\n",
    "\n",
    "# 캡션 출력\n",
    "# TODO : print 함수를 사용하여 캡션을 출력해주세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvSp1iWTKTTM"
   },
   "source": [
    "## 1-4. Metadata.csv 작성하기[과제]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1OwT4U-KTTM"
   },
   "source": [
    "이러한 Caption들을 지정된 format에 맞추어 csv 혹은 json 포맷으로 저장해야합니다.  \n",
    "csv 포맷이라 하면 아래와 같이  \n",
    "\n",
    "    file_name,additional_feature\n",
    "    0001.png,This is a first value of a text feature you added to your images\n",
    "    0002.png,This is a second value of a text feature you added to your images\n",
    "    0003.png,This is a third value of a text feature you added to your images\n",
    "\n",
    "json 포맷이라 하면  \n",
    "\n",
    "    {\"file_name\": \"0001.png\", \"additional_feature\": \"This is a first value of a text feature you added to your images\"}\n",
    "    {\"file_name\": \"0002.png\", \"additional_feature\": \"This is a second value of a text feature you added to your images\"}\n",
    "    {\"file_name\": \"0003.png\", \"additional_feature\": \"This is a third value of a text feature you added to your images\"}\n",
    "\n",
    "와 같이 작성해야합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veYB3zvgKTTM"
   },
   "source": [
    "[과제] 이를 위해 pandas dataframe을 이용하여 caption을 저장하고 metadata.csv 파일을 작성해보세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NDA2VbiKTTM"
   },
   "outputs": [],
   "source": [
    "# metadata 정의 column으로는 file_name, caption이 들어가야 합니다.\n",
    "metadata = None # TODO : pandas의 DataFrame Class를 사용하여 metadata를 정의해주세요. metadata에는 file_name과 caption이 들어가야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Burt_RAMKTTM"
   },
   "outputs": [],
   "source": [
    "for idx, item in enumerate(\n",
    "    os.listdir(os.path.join(custom_dataset_folder_path, \"train\"))\n",
    "):\n",
    "    format = item.split(\".\")[-1]\n",
    "    image_path = os.path.join(custom_dataset_folder_path, \"train\", item)\n",
    "    test_image = Image.open(image_path)\n",
    "    inputs = None # 여기에 코드를 작성해주세요\n",
    "    out = None # 여기에 코드를 작성해주세요\n",
    "    caption = None # 여기에 코드를 작성해주세요\n",
    "    metadata.loc[idx] = [item, caption]\n",
    "\n",
    "# 작성히 완료된 metadat는 file_name 순으로 정렬합니다.\n",
    "metadata = metadata.sort_values(by=[\"file_name\"]).reset_index(drop=True)\n",
    "\n",
    "# metadata를 csv 파일로 저장합니다.\n",
    "\n",
    "metadata.to_csv(\n",
    "    os.path.join(custom_dataset_folder_path, \"train\", \"metadata.csv\"), index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcjLBSyaKTTM"
   },
   "source": [
    "최종 폴더구조는 아래와 같이 이루어지게 됩니다\n",
    "<pre><code>\n",
    "├── gen_ai_custom_dataset\n",
    "│   └── train\n",
    "│   │    └── metadata.csv\n",
    "│   │    └── 00.jpg\n",
    "...\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuvX048nKTTM"
   },
   "source": [
    "## 1-5. Dataset 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMyjFz6-KTTM"
   },
   "source": [
    "위와 같은 과정이 마무리 된다면, 이제 customdataset을 정의할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdMqja8KKTTN"
   },
   "outputs": [],
   "source": [
    "custom_dataset = load_dataset(\"imagefolder\", data_dir=custom_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gb5rHkg8KTTN"
   },
   "source": [
    "## 2. LoRA 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhbfVRyxKTTN"
   },
   "source": [
    "#### Prompt Tokenization 을 위한 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKG69MNTKTTN"
   },
   "outputs": [],
   "source": [
    "def tokenize_prompt(tokenizer, prompt):\n",
    "    text_inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    ")\n",
    "    text_input_ids = text_inputs.input_ids\n",
    "    return text_input_ids\n",
    "\n",
    "\n",
    "def encode_prompt(text_encoders, tokenizers, prompt, text_input_ids_list=None):\n",
    "    prompt_embeds_list = []\n",
    "\n",
    "    for i, text_encoder in enumerate(text_encoders):\n",
    "        if tokenizers is not None:\n",
    "            tokenizer = tokenizers[i]\n",
    "            text_input_ids = tokenize_prompt(tokenizer, prompt)\n",
    "        else:\n",
    "            assert text_input_ids_list is not None\n",
    "            try:\n",
    "                text_input_ids = text_input_ids_list[i]\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        prompt_embeds = text_encoder(\n",
    "            text_input_ids.to(text_encoder.device),\n",
    "            output_hidden_states=True,\n",
    "            return_dict=False,\n",
    "    )\n",
    "\n",
    "        pooled_prompt_embeds = prompt_embeds[0]\n",
    "        prompt_embeds = prompt_embeds[-1][-2]\n",
    "        bs_embed, seq_len, _ = prompt_embeds.shape\n",
    "        prompt_embeds = prompt_embeds.view(bs_embed, seq_len, -1)\n",
    "        prompt_embeds_list.append(prompt_embeds)\n",
    "\n",
    "    prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)\n",
    "    pooled_prompt_embeds = pooled_prompt_embeds.view(bs_embed, -1)\n",
    "    return prompt_embeds, pooled_prompt_embeds\n",
    "\n",
    "def import_model_class_from_model_name_or_path(\n",
    "    pretrained_model_name_or_path: str, subfolder: str = \"text_encoder\"\n",
    "):\n",
    "    text_encoder_config = PretrainedConfig.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=subfolder\n",
    ")\n",
    "    model_class = text_encoder_config.architectures[0]\n",
    "\n",
    "    if model_class == \"CLIPTextModel\":\n",
    "        from transformers import CLIPTextModel\n",
    "\n",
    "        return CLIPTextModel(text_encoder_config).from_pretrained(\n",
    "            pretrained_model_name_or_path, subfolder=subfolder\n",
    "        )\n",
    "    elif model_class == \"CLIPTextModelWithProjection\":\n",
    "        from transformers import CLIPTextModelWithProjection\n",
    "\n",
    "        return CLIPTextModelWithProjection(text_encoder_config).from_pretrained(\n",
    "            pretrained_model_name_or_path, subfolder=subfolder\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"{model_class} is not supported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrIgBZS-KTTN"
   },
   "source": [
    "### StableDiffusion을 구성하는 세부 모델들 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLAgHeQ1KTTN"
   },
   "source": [
    "#### Pretrained Model 및 기타 config 선언\n",
    "학습에 사용될 pretrained model은 \"runwayml/stable-diffusion-v1-5\" 을 이용합니다.  \n",
    "빠른 학습을 위해 Floating Point Precision (부동소수점) 은 16 FP16 을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "poktxEF8KTTN"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME  = \"runwayml/stable-diffusion-v1-5\"\n",
    "VAE_NAME =\"stabilityai/sd-vae-ft-mse\"\n",
    "VARIANT = \"fp16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6sf5nmMKTTO"
   },
   "source": [
    "#### [과제] Tokenizer, Text Encoder 선언\n",
    "\n",
    "Prompt를 사용하기 위해서는 Tokenizer와 Text Encoder를 선언해야합니다.\n",
    "아래 빈 부분을 작성하여 Tokenizer와 Text Encoder를 선언해보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUJGsH-XKTTO"
   },
   "outputs": [],
   "source": [
    "tokenizer_one = AutoTokenizer.from_pretrained(\n",
    "        # TODO : pretrained_model_name_or_path 값을 정의해주세요.\n",
    "        # TODO : subfolder 값을 정의해주세요.\n",
    "        # TODO : use_fast 옵션의 값을 정의해주세요.\n",
    ")\n",
    "text_encoder_one = import_model_class_from_model_name_or_path(\n",
    "        # TODO : pretrained_model_name_or_path 값을 정의해주세요.\n",
    "        # TODO : subfolder 값을 정의해주세요.\n",
    ")\n",
    "\n",
    "tokenize_two, text_encoder_two = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URkJbv9CKTTO"
   },
   "source": [
    "#### [과제] Noise Scheduler, VAE, UNET 선언\n",
    "\n",
    "아래 빈 부분을 작성하여 Diffusion 모델을 구성하는 noise scheduler, VAE, UNET을 선언해보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VJMZVDjKTTO"
   },
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "        # TODO : pretrained_model_name_or_path 값을 정의해주세요.\n",
    "        # TODO : subfolder 값을 정의해주세요.\n",
    ")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "        # TODO : pretrained_model_name_or_path 값을 정의해주세요.\n",
    ")\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "        # TODO : pretrained_model_name_or_path 값을 정의해주세요.\n",
    "        # TODO : subfolder 값을 정의해주세요.\n",
    "        # TODO : variant 값을 정의해주세요.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuZqutAdKTTP"
   },
   "source": [
    "추가된 adapter LoRA 레이어들만 학습 시키기 때문 나머지 함수들은 requires_grad_(False)로 설정해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36iIEBPfKTTP"
   },
   "outputs": [],
   "source": [
    "vae.requires_grad_(False)\n",
    "text_encoder_one.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "print(\"VAE, Text Encoder, and UNet are frozen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WURQHLAKKTTP"
   },
   "source": [
    "### 학습 루프 함수 정의\n",
    "- 학습 Step 수는 10,000 Step을 권장하나, 학습 소요 시간이 오래 걸릴 경우, Step 수를 줄여 학습을 진행하시길 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMqvud9gKTTP"
   },
   "outputs": [],
   "source": [
    "seed = 2015\n",
    "train_batch_size = 2\n",
    "resolution = 1024\n",
    "max_train_steps = 10000\n",
    "checkpointing_steps = 5000\n",
    "learning_rate = 1e-4\n",
    "output_dir = \"./sd_lora\"\n",
    "\n",
    "# Make sure output_dir exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-JrFaG5KTTP"
   },
   "outputs": [],
   "source": [
    "logger = get_logger(__name__)\n",
    "\n",
    "def training_function(dataset, unet, vae, text_encoder_one, text_encoder_two=None):\n",
    "\n",
    "    # Init Accelerator\n",
    "    accelerator = Accelerator(mixed_precision=VARIANT)\n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "\n",
    "    # 세부 모델들 GPU로 올리기, 및 datatype 설정\n",
    "    unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    text_encoder_one.to(accelerator.device, dtype=weight_dtype)\n",
    "    if text_encoder_two is not None:\n",
    "        text_encoder_two.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "    # LoRA config 설정\n",
    "    # UNet의 Attention Layer에 LoRA weights를 추가\n",
    "    # Set correct lora layers\n",
    "    unet_lora_config = LoraConfig(\n",
    "        r=4, # Dimension of LoRA update Matrices\n",
    "        lora_alpha=4, # Dimension of LoRA update Matrices\n",
    "        init_lora_weights=\"gaussian\",\n",
    "        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
    "    )\n",
    "    unet.add_adapter(unet_lora_config)\n",
    "\n",
    "    # Trainable Parameter (LoRA weights) 만을 FP32로 Upcast\n",
    "    cast_training_params([unet], dtype=torch.float32)\n",
    "\n",
    "    # Accelerator에 wrap된 모델을 unwrap하는 내부함수\n",
    "    def unwrap_model(model):\n",
    "        model = accelerator.unwrap_model(model)\n",
    "        model = model._orig_mod if is_compiled_module(model) else model\n",
    "        return model\n",
    "\n",
    "\n",
    "    # Custom 된  save & loading hook 함수. LoRA Weight만을 저장하고 불러오는 용도로 사용\n",
    "    # 실질적으로 accelrator.save_state() 함수 사용을 위해 필요한 함수\n",
    "    def save_model_hook(models, weights, output_dir):\n",
    "        if accelerator.is_main_process:\n",
    "            # there are only two options here. Either are just the unet attn processor layers\n",
    "            # or there are the unet and text encoder atten layers\n",
    "            unet_lora_layers_to_save = None\n",
    "            text_encoder_one_lora_layers_to_save = None\n",
    "            if text_encoder_two is not None:\n",
    "                text_encoder_two_lora_layers_to_save = None\n",
    "\n",
    "            for model in models:\n",
    "                if isinstance(unwrap_model(model), type(unwrap_model(unet))):\n",
    "                    unet_lora_layers_to_save = convert_state_dict_to_diffusers(\n",
    "                        get_peft_model_state_dict(model)\n",
    "                    )\n",
    "                elif isinstance(\n",
    "                    unwrap_model(model), type(unwrap_model(text_encoder_one))\n",
    "                ):\n",
    "                    text_encoder_one_lora_layers_to_save = (\n",
    "                        convert_state_dict_to_diffusers(\n",
    "                            get_peft_model_state_dict(model)\n",
    "                        )\n",
    "                    )\n",
    "                elif isinstance(\n",
    "                    unwrap_model(model), type(unwrap_model(text_encoder_two))\n",
    "                ):\n",
    "                    text_encoder_two_lora_layers_to_save = (\n",
    "                        convert_state_dict_to_diffusers(\n",
    "                            get_peft_model_state_dict(model)\n",
    "                        )\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"unexpected save model: {model.__class__}\")\n",
    "\n",
    "                # make sure to pop weight so that corresponding model is not saved again\n",
    "                if weights:\n",
    "                    weights.pop()\n",
    "\n",
    "            if text_encoder_two is not None:\n",
    "                StableDiffusionXLPipeline.save_lora_weights(\n",
    "                    output_dir,\n",
    "                    unet_lora_layers=unet_lora_layers_to_save,\n",
    "                    text_encoder_lora_layers=text_encoder_one_lora_layers_to_save,\n",
    "                    text_encoder_2_lora_layers=text_encoder_two_lora_layers_to_save,\n",
    "                )\n",
    "            else:\n",
    "                StableDiffsuionPipeline.save_lora_weights(\n",
    "                    output_dir,\n",
    "                    unet_lora_layers=unet_lora_layers_to_save,\n",
    "                    text_encoder_lora_layers=text_encoder_one_lora_layers_to_save,\n",
    "                )\n",
    "\n",
    "    def load_model_hook(models, input_dir):\n",
    "        unet_ = None\n",
    "        text_encoder_one_ = None\n",
    "        text_encoder_two_ = None\n",
    "\n",
    "        while len(models) > 0:\n",
    "            model = models.pop()\n",
    "\n",
    "            if isinstance(model, type(unwrap_model(unet))):\n",
    "                unet_ = model\n",
    "            elif isinstance(model, type(unwrap_model(text_encoder_one))):\n",
    "                text_encoder_one_ = model\n",
    "            elif isinstance(model, type(unwrap_model(text_encoder_two))):\n",
    "                text_encoder_two_ = model\n",
    "            else:\n",
    "                raise ValueError(f\"unexpected save model: {model.__class__}\")\n",
    "\n",
    "        lora_state_dict, network_alphas = LoraLoaderMixin.lora_state_dict(input_dir)\n",
    "        LoraLoaderMixin.load_lora_into_unet(\n",
    "            lora_state_dict, network_alphas=network_alphas, unet=unet_\n",
    "        )\n",
    "\n",
    "        text_encoder_state_dict = {\n",
    "            k: v for k, v in lora_state_dict.items() if \"text_encoder.\" in k\n",
    "        }\n",
    "        LoraLoaderMixin.load_lora_into_text_encoder(\n",
    "            text_encoder_state_dict,\n",
    "            network_alphas=network_alphas,\n",
    "            text_encoder=text_encoder_one_,\n",
    "        )\n",
    "        if text_encoder_two_ is not None:\n",
    "            text_encoder_2_state_dict = {\n",
    "                k: v for k, v in lora_state_dict.items() if \"text_encoder_2.\" in k\n",
    "            }\n",
    "            LoraLoaderMixin.load_lora_into_text_encoder(\n",
    "                text_encoder_2_state_dict,\n",
    "                network_alphas = network_alphas,\n",
    "                text_encoder = text_encoder_two_,\n",
    "            )\n",
    "\n",
    "    accelerator.register_save_state_pre_hook(save_model_hook)\n",
    "    accelerator.register_load_state_pre_hook(load_model_hook)\n",
    "\n",
    "    # Optimizer 설정\n",
    "    optimizer_class = torch.optim.AdamW\n",
    "    # Optimizer creation\n",
    "    params_to_optimize = list(filter(lambda p: p.requires_grad, unet.parameters()))\n",
    "    optimizer = optimizer_class(\n",
    "        params_to_optimize,\n",
    "        lr=learning_rate*train_batch_size,\n",
    "        betas = (0.9, 0.999),\n",
    "        weight_decay = 1e-2,\n",
    "        eps = 1e-08,\n",
    "    )\n",
    "\n",
    "    # Preprocessing the datasets\n",
    "    # 먼저 데이터셋의 Column 구분 (Image, Caption)\n",
    "    column_names = dataset[\"train\"].column_names\n",
    "    image_column = column_names[0]\n",
    "    caption_column = column_names[1]\n",
    "\n",
    "   # Captipon 들을 두가지 Text Tokenizer로 Tokenize 하는 내부함수 정의\n",
    "    def tokenize_captions(examples, is_train=True,):\n",
    "        captions = []\n",
    "        for caption in examples[caption_column]:\n",
    "            if isinstance(caption, str):\n",
    "                captions.append(caption)\n",
    "            elif isinstance(caption, (list, np.ndarray)):\n",
    "                # take a random caption if there are multiple\n",
    "                captions.append(random.choice(caption) if is_train else caption[0])\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n",
    "                )\n",
    "        tokens_one = tokenize_prompt(tokenizer_one, captions)\n",
    "        if text_encoder_two is not None:\n",
    "            tokens_two = tokenize_prompt(tokenizer_two, captions)\n",
    "        else:\n",
    "            tokens_two = None\n",
    "        return tokens_one, tokens_two\n",
    "\n",
    "    # Image 를 전처리 하는 부분 정의\n",
    "    train_resize = transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "    train_crop = transforms.CenterCrop(resolution)\n",
    "    train_flip = transforms.RandomHorizontalFlip(p=1.0)\n",
    "    train_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Image 와 Caption preprocessing 을 위한 종합 내부함수정의\n",
    "    def preprocess_train(examples):\n",
    "        images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
    "        # image aug\n",
    "        original_sizes = []\n",
    "        all_images = []\n",
    "        crop_top_lefts = []\n",
    "        for image in images:\n",
    "            original_sizes.append((image.height, image.width))\n",
    "            image = train_resize(image)\n",
    "            if random.random() < 0.5:# flip\n",
    "                image = train_flip(image)\n",
    "            y1 = max(0, int(round((image.height - resolution) / 2.0)))\n",
    "            x1 = max(0, int(round((image.width - resolution) / 2.0)))\n",
    "            image = train_crop(image)\n",
    "            crop_top_left = (y1, x1)\n",
    "            crop_top_lefts.append(crop_top_left)\n",
    "            image = train_transforms(image)\n",
    "            all_images.append(image)\n",
    "\n",
    "        examples[\"original_sizes\"] = original_sizes\n",
    "        examples[\"crop_top_lefts\"] = crop_top_lefts\n",
    "        examples[\"pixel_values\"] = all_images\n",
    "        tokens_one, tokens_two = tokenize_captions(examples)\n",
    "        examples[\"input_ids_one\"] = tokens_one\n",
    "        if text_encoder_two is not None:\n",
    "            examples[\"input_ids_two\"] = tokens_two\n",
    "        return examples\n",
    "\n",
    "    # Training Dataset 생성\n",
    "    train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n",
    "\n",
    "    # DataLoader 생성\n",
    "    def collate_fn(examples):\n",
    "        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "        original_sizes = [example[\"original_sizes\"] for example in examples]\n",
    "        crop_top_lefts = [example[\"crop_top_lefts\"] for example in examples]\n",
    "        input_ids_one = torch.stack([example[\"input_ids_one\"] for example in examples])\n",
    "        if text_encoder_two is not None:\n",
    "            input_ids_two = torch.stack([example[\"input_ids_two\"] for example in examples])\n",
    "            return {\n",
    "                \"pixel_values\": pixel_values,\n",
    "                \"input_ids_one\": input_ids_one,\n",
    "                \"input_ids_two\": input_ids_two,\n",
    "                \"original_sizes\": original_sizes,\n",
    "                \"crop_top_lefts\": crop_top_lefts,\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"pixel_values\": pixel_values,\n",
    "                \"input_ids_one\": input_ids_one,\n",
    "                \"original_sizes\": original_sizes,\n",
    "                \"crop_top_lefts\": crop_top_lefts,\n",
    "            }\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle = True,\n",
    "        collate_fn = collate_fn,\n",
    "        batch_size = train_batch_size,\n",
    "        num_workers = 0,\n",
    "    )\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        'cosine',\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps = 500 * 1,\n",
    "        num_training_steps = max_train_steps *1,\n",
    "    )\n",
    "\n",
    "    # Prepare with  accelerator\n",
    "    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(unet, optimizer, train_dataloader, lr_scheduler)\n",
    "\n",
    "\n",
    "    # Training 시작 전 기본 세팅\n",
    "    num_train_epochs = math.ceil(max_train_steps / len(train_dataset))\n",
    "    total_batch_size = train_batch_size * accelerator.num_processes\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num Epochs = {num_train_epochs}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {train_batch_size}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "    logger.info(f\"  Total optimization steps = {max_train_steps}\")\n",
    "    global_step = 0\n",
    "    first_epoch = 0\n",
    "    initial_global_step = 0\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        range(0, max_train_steps),\n",
    "        initial = initial_global_step,\n",
    "        desc=\"Steps\",\n",
    "        # Only show the progress bar once on each machine.\n",
    "        disable = not accelerator.is_local_main_process,\n",
    "    )\n",
    "\n",
    "    # Training Loop 시작\n",
    "    for epoch in range(first_epoch, num_train_epochs):\n",
    "        unet.train()\n",
    "        train_loss = 0.0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            with accelerator.accumulate(unet):\n",
    "                # Convert images to latent space\n",
    "                pixel_values = batch[\"pixel_values\"].to(dtype=weight_dtype)\n",
    "                model_input = vae.encode(pixel_values).latent_dist.sample()\n",
    "                model_input = model_input * vae.config.scaling_factor\n",
    "\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn_like(model_input)\n",
    "\n",
    "                bsz = model_input.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(\n",
    "                    0,\n",
    "                    noise_scheduler.config.num_train_timesteps,\n",
    "                    (bsz,),\n",
    "                    device=model_input.device,\n",
    "                )\n",
    "                timesteps = timesteps.long()\n",
    "\n",
    "                # Add noise to the model input according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_model_input = noise_scheduler.add_noise(model_input, noise, timesteps)\n",
    "\n",
    "                # time ids\n",
    "                def compute_time_ids(original_size, crops_coords_top_left):\n",
    "                    # Adapted from pipeline.StableDiffusionXLPipeline._get_add_time_ids\n",
    "                    target_size = (resolution, resolution)\n",
    "                    add_time_ids = list(original_size + crops_coords_top_left + target_size)\n",
    "                    add_time_ids = torch.tensor([add_time_ids])\n",
    "                    add_time_ids = add_time_ids.to(accelerator.device, dtype=weight_dtype)\n",
    "                    return add_time_ids\n",
    "\n",
    "                add_time_ids = torch.cat(\n",
    "                    [\n",
    "                        compute_time_ids(s, c)\n",
    "                        for s, c in zip(\n",
    "                            batch[\"original_sizes\"], batch[\"crop_top_lefts\"]\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                # Predict the noise residual\n",
    "                unet_added_conditions = {\"time_ids\": add_time_ids}\n",
    "\n",
    "                if text_encoder_two is not None:\n",
    "                    text_input_ids_list = [\n",
    "                        batch[\"input_ids_one\"],\n",
    "                        batch[\"input_ids_two\"],\n",
    "                    ]\n",
    "                    prompt_embeds, pooled_prompt_embeds = encode_prompt(\n",
    "                        text_encoders=[text_encoder_one, text_encoder_two],\n",
    "                        tokenizers=None,\n",
    "                        prompt=None,\n",
    "                        text_input_ids_list=text_input_ids_list,\n",
    "                    )\n",
    "                else:\n",
    "                    text_input_ids_list = [batch[\"input_ids_one\"]]\n",
    "\n",
    "                    prompt_embeds, pooled_prompt_embeds = encode_prompt(\n",
    "                        text_encoders=[text_encoder_one],\n",
    "                        tokenizers=None,\n",
    "                        prompt=None,\n",
    "                        text_input_ids_list=text_input_ids_list,\n",
    "                    )\n",
    "                unet_added_conditions.update({\"text_embeds\": pooled_prompt_embeds})\n",
    "                model_pred = unet(\n",
    "                    noisy_model_input,\n",
    "                    timesteps,\n",
    "                    prompt_embeds,\n",
    "                    added_cond_kwargs=unet_added_conditions,\n",
    "                    return_dict=False,\n",
    "                )[0]\n",
    "\n",
    "\n",
    "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                    target = noise\n",
    "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                    target = noise_scheduler.get_velocity(model_input, noise, timesteps)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Unknown prediction type {noise_scheduler.config.prediction_type}\"\n",
    "                    )\n",
    "\n",
    "                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "\n",
    "                # Gather the losses across all processes for logging (if we use distributed training).\n",
    "                avg_loss = accelerator.gather(loss.repeat(train_batch_size)).mean()\n",
    "                train_loss += avg_loss.item()\n",
    "\n",
    "                # Backpropagate\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(params_to_optimize, 1)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "                accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
    "                train_loss = 0.0\n",
    "\n",
    "                if accelerator.is_main_process:\n",
    "                    if global_step % checkpointing_steps == 0:\n",
    "                        save_path = os.path.join(\n",
    "                            output_dir, f\"checkpoint-{global_step}\"\n",
    "                        )\n",
    "                        accelerator.save_state(save_path)\n",
    "                        logger.info(f\"Saved state to {save_path}\")\n",
    "\n",
    "            logs = {\n",
    "                \"step_loss\": loss.detach().item(),\n",
    "                \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "            }\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if global_step >= max_train_steps:\n",
    "                break\n",
    "    # Training 종료\n",
    "    # Save the lora layers\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        unet = unwrap_model(unet)\n",
    "        unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unet))\n",
    "        StableDiffusionXLPipeline.save_lora_weights(\n",
    "            save_directory=output_dir,\n",
    "            unet_lora_layers=unet_lora_state_dict,\n",
    "        )\n",
    "        del unet\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLHU8nPKKTTQ"
   },
   "outputs": [],
   "source": [
    "import accelerate\n",
    "\n",
    "accelerate.notebook_launcher(training_function, args=(custom_dataset, unet, vae, text_encoder_one), num_processes=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prAvhuhzKTTQ"
   },
   "source": [
    "## [과제] Load 학습된 LoRA\n",
    "\n",
    "학습된 LoRA 모델을 불러와서 inference를 수행해보세요\n",
    "이를 위해 pipeline을 정의하고, 학습한 lora weight를 불러와 이미지를 생성해보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTG0tSj7KTTQ"
   },
   "outputs": [],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "                                                # TODO : pretrained_model_name_or_path 값을 정의해주세요.\n",
    "                                                # TODO : torch_dtype 값을 정의해주세요.\n",
    "                                                # TODO : variant 값을 정의해주세요.\n",
    "                                                # TODO : use_safetensors 옵션을 정의해주세요.\n",
    "                                                )\n",
    "pipeline = pipeline.to(device)\n",
    "print(\"pipeline loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNkxoXGLKTTR"
   },
   "outputs": [],
   "source": [
    "# \"./sd_lora_custom\"에 저장된 LoRA weights를 불러와보세요.\n",
    "# TODO : pipline class 내부 함수인 load_lora_weights를 사용하여 \"./sd_lora_custom\"에 저장된 LoRA weights를 불러와보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvspx_zMKTTR"
   },
   "outputs": [],
   "source": [
    "inference_prompt = \"8bit illustration of city view\"\n",
    "images = pipeline(inference_prompt, num_images_per_prompt=3).images\n",
    "make_image_grid(images, rows=1, cols=3)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "generative_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
