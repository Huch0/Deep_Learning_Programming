{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCuQfV0vvMzI"
   },
   "source": [
    "#LAB 4: Semantic segmentation\n",
    "\n",
    "<h4><div style=\"text-align: right\"> Due date: 15:00 Nov 18, 2024.  </div> <br>\n",
    "<div style=\"text-align: right\"> Please upload your file and final-report at PLATO before the class in the form of [ID_Name_Lab1.ipynb]. </div></h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8mu4LKwvMzJ"
   },
   "source": [
    "### *Instructions:*\n",
    "- Write a program implementing a particular algorithm to solve a given problem.   \n",
    "- <span style=\"color:red\">**Report and discuss your results. Analyze the algorithm, theoretically and empirically.**</span>\n",
    "- You must write their own answers and codes (<span style=\"color:red\">**if not you will get a F grade**</span>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IGK6e0SvMzK"
   },
   "source": [
    "\n",
    "<h2><span style=\"color:blue\">[202055623] [허치영]</span> </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTknvfoLvMzL"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(\"This code is written at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpSYkxcHvMzO"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FODV6y3hvMzQ"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uR60cFKkvMzR"
   },
   "source": [
    "### What is Semantic Segmentation?\n",
    "#### Semantic segmentation is an approach to understand what is in the image in pixel-level:\n",
    "\n",
    "- It is a lot more difficult than image classification, which makes a prediction in image-level.\n",
    "\n",
    "- It differs from object detection in that it has no information about instances.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Oe6OVIuT8WPxK4x0LXOunxL-tVtzg948\" alt=\"no_image\" style=\"width: 900px;\"/>\n",
    "\n",
    "Applications for semantic segmentation include:\n",
    "\n",
    "- Autonomous driving\n",
    "\n",
    "- Image Editing\n",
    "\n",
    "- Classification of terrain visible in satellite imagery\n",
    "\n",
    "- Medical imaging analysis\n",
    "\n",
    "### 1. Semantic segmentation with CNNs\n",
    "- Typical classification models (AlexNet, VGGNet, ...) take fixed-sized inputs and produce a probability vector. The fully connected layers of these models have fixed dimensions and throw away spatial coordinates.\n",
    "\n",
    "\n",
    "- It is known that the fully connected layer can be viewed as a convolution layer with a kernel that covers only one pixel, that is, a 1x1 kernel. Thus, we can convert the fully connected layers into convolution layers with maintaining pre-trained weights.\n",
    "\n",
    "\n",
    "- After 'convolutionalizing' fully connected layers, a feature map needs to be upsampled because of pooling operations in the models. Instead of using simple bilinear interpolation, we can use a transposed convolution layer to learn the interpolation process. This layer is also called as upconvolution, deconvolution or fractionally-strided convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgYW4kBcvMzS"
   },
   "source": [
    "### 1.1 Pixel wise classification using sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQakEYrgvMzT"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1RJOmD22NyRT-6T0hCQ5hbU3FB9Mpevyu\" alt=\"no_image\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaT7PMWhvMzT"
   },
   "source": [
    "### 1.1.1 Example with VGG-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NogQFAoSvMzU"
   },
   "source": [
    "**Load a test image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7MRNov26vMzU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "test_img_path = 'lab4/img/2009_005160.jpg'\n",
    "test_img = Image.open(test_img_path)\n",
    "test_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY7b3z4lvMzZ"
   },
   "source": [
    "**Standardization of the test image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6ai-rWmvMzZ"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "test_transform = transform(test_img).cuda().unsqueeze(0)\n",
    "print(test_transform.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mr_vqOzFvMzc"
   },
   "source": [
    "**Load VGG-Net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNScLN6mvMzd"
   },
   "outputs": [],
   "source": [
    "import torchvision.models.vgg as vgg\n",
    "imageNet = vgg.vgg16(pretrained=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3i6_6DWvMzg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "with torch.no_grad():\n",
    "    out = imageNet(test_transform)\n",
    "\n",
    "out_class = torch.argmax(out)\n",
    "print(out.size())\n",
    "print(out_class) # 285 is egyptian cat in ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvwIjZc4vMzj"
   },
   "outputs": [],
   "source": [
    "# padding\n",
    "m = nn.ZeroPad2d((111,112,111,112))\n",
    "pad_image = m(test_transform)\n",
    "res = torch.zeros((224,224)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2blOceRvMzl"
   },
   "outputs": [],
   "source": [
    "# sliding window approach for segmentation\n",
    "# NOTICE: It takes some time\n",
    "for i in range(224):\n",
    "    for j in range(224):\n",
    "        patch = pad_image[:,:,i:i+224,j:j+224]\n",
    "        # classify each pixels\n",
    "        with torch.no_grad():\n",
    "            res[i,j] = torch.argmax(imageNet(patch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTzCjKwTvMzn"
   },
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVmoWL7OvMzq"
   },
   "source": [
    "**Visualize the output, classified as 'egyptian cat' (285)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zI0M6IakvMzr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_transform[0].data.cpu().numpy().transpose((1,2,0)))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow((res==285).data.cpu().numpy()) # Visualize pixels classified as egyptian cat\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yImySwCjvMzw"
   },
   "source": [
    "- Inefficient & Ineffective !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NzE9PbnvMzz"
   },
   "source": [
    "### 1.2 Pixel wise classification using image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coYohYolvMzz"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1XaspEEpplLzpNJ8qJpPN24qMeVmbsp5P\" alt=\"no_image\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIFpgdQMvMz0"
   },
   "source": [
    "#### 1.2.1 Convolutional VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EDQ5VGbvMz0"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class ConvolutionalVGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalVGG, self).__init__()\n",
    "        self.features = models.vgg16(pretrained=True).features\n",
    "\n",
    "        # fc6\n",
    "        self.fc6 = nn.Conv2d(512, 4096, 7)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.drop6 = nn.Dropout2d()\n",
    "\n",
    "        # fc7\n",
    "        self.fc7 = nn.Conv2d(4096, 4096, 1)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.drop7 = nn.Dropout2d()\n",
    "\n",
    "        # fc8\n",
    "        self.fc8 = nn.Conv2d(4096, 1000, 1)\n",
    "\n",
    "        self.copy_params_from_vgg16()\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv5 = self.features(x)\n",
    "\n",
    "        fc6 = self.relu6(self.fc6(conv5))\n",
    "        fc7 = self.drop6(fc6)\n",
    "\n",
    "        fc7 = self.relu7(self.fc7(fc7))\n",
    "        fc8 = self.drop7(fc7)\n",
    "\n",
    "        score = self.fc8(fc8)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def copy_params_from_vgg16(self):\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        for i, name in zip([0, 3, 6], ['fc6', 'fc7', 'fc8']):\n",
    "            l1 = vgg16.classifier[i]\n",
    "            l2 = getattr(self, name)\n",
    "            l2.weight.data.copy_(l1.weight.data.view(l2.weight.size()))\n",
    "            l2.bias.data.copy_(l1.bias.data.view(l2.bias.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-dMZ3Q4vMz4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conv_vgg = ConvolutionalVGG().cuda()\n",
    "conv_vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79XmaVz0vMz7"
   },
   "outputs": [],
   "source": [
    "transform_conv = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlqZgmttvM0A"
   },
   "outputs": [],
   "source": [
    "test_conv = transform_conv(test_img).cuda().unsqueeze(0)\n",
    "print(test_conv.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vt2HDu14vM0D"
   },
   "outputs": [],
   "source": [
    "conv_out = conv_vgg(test_conv)\n",
    "pred = torch.argmax(conv_out, dim=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fY9MAjhGvM0G"
   },
   "source": [
    "Well...\n",
    "\n",
    "Although modifying VGG-Net to fully convolutional network results in a pixel-wise ouput, the output resolution is smaller than the original image resolution. Moreover, we can see that many pixels are misclassified (i.e., not 285)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWwnJEC7vM0G"
   },
   "source": [
    "#### 1.2.2 Add simple upsampling layer (Bilinear) and retrain with PASCAL VOC 2012  [1 point]\n",
    "- Re-define the last layer so as to classify classes in the PASCAL VOC datasets\n",
    "- Use bilinear interpolation to make the network output equal to the input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1fI38HwnvM0H"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class ConvolutionalVGGwithUpsample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalVGGwithUpsample, self).__init__()\n",
    "        self.features = models.vgg16(pretrained=True).features\n",
    "        self.features[0].padding = (100,100)\n",
    "\n",
    "        # fc6\n",
    "        self.fc6 = nn.Conv2d(512, 4096, 7)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.drop6 = nn.Dropout2d()\n",
    "\n",
    "        # fc7\n",
    "        self.fc7 = nn.Conv2d(4096, 4096, 1)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.drop7 = nn.Dropout2d()\n",
    "\n",
    "        # fc8\n",
    "        #############\n",
    "        # CODE HERE #\n",
    "        #############\n",
    "\n",
    "        self.copy_params_from_vgg16()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv5 = self.features(x)\n",
    "\n",
    "        fc6 = self.relu6(self.fc6(conv5))\n",
    "        fc7 = self.drop6(fc6)\n",
    "\n",
    "        fc7 = self.relu7(self.fc7(fc7))\n",
    "        fc8 = self.drop7(fc7)\n",
    "\n",
    "        score = self.fc8(fc8)\n",
    "\n",
    "        #############\n",
    "        # CODE HERE #\n",
    "        #############\n",
    "        # Upsample to original resolution\n",
    "\n",
    "        return score\n",
    "\n",
    "    def copy_params_from_vgg16(self):\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        for i, name in zip([0, 3], ['fc6', 'fc7']):\n",
    "            l1 = vgg16.classifier[i]\n",
    "            l2 = getattr(self, name)\n",
    "            l2.weight.data.copy_(l1.weight.data.view(l2.weight.size()))\n",
    "            l2.bias.data.copy_(l1.bias.data.view(l2.bias.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yv5Rd8APvM0J"
   },
   "outputs": [],
   "source": [
    "conv_vgg_upsample = ConvolutionalVGGwithUpsample().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8Z4cwrBvM0L"
   },
   "outputs": [],
   "source": [
    "def decode_labels(mask, num_classes=21):\n",
    "    from PIL import Image\n",
    "    label_colours = [(0, 0, 0)\n",
    "                 # 0=background\n",
    "                 , (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128)\n",
    "                 # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle\n",
    "                 , (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0)\n",
    "                 # 6=bus, 7=car, 8=cat, 9=chair, 10=cow\n",
    "                 , (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128)\n",
    "                 # 11=diningtable, 12=dog, 13=horse, 14=motorbike, 15=person\n",
    "                 , (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)]\n",
    "                 # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n",
    "\n",
    "    h, w = mask.shape\n",
    "\n",
    "    img = Image.new('RGB', (w, h))\n",
    "    pixels = img.load()\n",
    "    for j_, j in enumerate(mask[:, :]):\n",
    "        for k_, k in enumerate(j):\n",
    "            if k < num_classes:\n",
    "                pixels[k_, j_] = label_colours[k]\n",
    "    output = np.array(img)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "spmo4ivPvM0M"
   },
   "outputs": [],
   "source": [
    "model_data = torch.load('lab4/vgg_conv_upsample.pth')\n",
    "conv_vgg_upsample.load_state_dict(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDz52cCgvM0O"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv_out = conv_vgg_upsample(test_conv)\n",
    "output = torch.argmax(conv_out, dim=1)\n",
    "\n",
    "vis_output = decode_labels(output[0].data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjB0rEFjvM0Q"
   },
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_conv[0].data.cpu().numpy().transpose((1,2,0)))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(vis_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-DNEX_vvM0S"
   },
   "source": [
    "### 1.3 Upsampling method with CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1f0MVyZrvM0S"
   },
   "source": [
    "#### 1.3.1 Transposed Convolution\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1olTrk2Hu1dAppF2zxFVedy0aIrAbYrAw\"  onerror=\"this.style.display='none'\" /><br><br>\n",
    "\n",
    "- The transposed convolution (it is often called as \"deconvolution\") is used to up-sample the input resolution by using learnable filters. In contrast to the standard convolution, which aggregates spatial information to a single point, it spreads a point of the input over multiple spatial locations.\n",
    "- For example, a $3\\times3$ kernel with a stride of 2 converts the~$2\\times2$~(cyan squares) input into the~$5\\times5$~(blue squares) output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWz_03fbvM0T"
   },
   "source": [
    "#### 1.3.2 Dilated Convolution\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1LxBcf7V_s7559Odw9JzPwXFpPYCXa4JT\"  onerror=\"this.style.display='none'\" /><br><br>\n",
    "- The dilated convolution introduces 'a dilation rate' to the standard convolution.\n",
    "\n",
    "- The dilation rate means a spacing value between elements of a kernel, which enlarges the receptive field without introducing additional parameters.\n",
    "\n",
    "- For example, a $3\\times3$ kernel with a dilation rate of 2 has the same receptive field as a $5\\times5$ kernel while it still use 9 parameters only, compared to the $5\\times5$ the uses 25 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzTej9BpvM0T"
   },
   "source": [
    "## 2. [Lab] Semantic segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3RtjdycvM0V"
   },
   "source": [
    "### 2.1 Write a FCN8s model  [4 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaNvWr3svM0V"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1xLdKvbMZrszjG2nVqFV3AsJ0t4M-nEN6\"  onerror=\"this.style.display='none'\" /><br><br>\n",
    "\n",
    "<p style='text-align:right;'>[<a href='https://medium.com/@wilburdes/semantic-segmentation-using-fully-convolutional-neural-networks-86e45336f99b'>source</a>]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nm0iMNEBvM0W"
   },
   "source": [
    "**Crop boundary example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EK_dTaMRvM0W"
   },
   "outputs": [],
   "source": [
    "a = torch.ones((1, 1, 160, 140))\n",
    "b = torch.ones((1, 1, 120, 120))\n",
    "\n",
    "try:\n",
    "    a + b\n",
    "except:\n",
    "    print('The size of tensors are different')\n",
    "    print(a.size())\n",
    "    print(b.size())\n",
    "\n",
    "# crop boundary\n",
    "a = a[:, :, 5: 5+b.size(2), 5:5+b.size(3)]\n",
    "\n",
    "# add connection with weight\n",
    "c = 0.01*a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9gyU7PBvM0Y"
   },
   "source": [
    "- Predict 1: $1\\times1$ Conv(in: 4096, out: n_class)\n",
    "- Predict 2: $1\\times1$ Conv(in: 512, out: n_class), weight = 0.01\n",
    "- Predict 3: $1\\times1$ Conv(in: 256, out: n_class), weight = 0.0001\n",
    "\n",
    "- Deconv 1: $4\\times4$ Transposed Conv (in: n_class, out: n_class, stride: 2, biase: False)\n",
    "- Deconv 2: $4\\times4$ Transposed Conv (in: n_class, out: n_class, stride: 2, biase: False)\n",
    "- Deconv 3: $16\\times16$ Transposed Conv (in: n_class, out: n_class, stride: 8, biase: False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Pk5AfkovM0Z"
   },
   "outputs": [],
   "source": [
    "class FCN8s(nn.Module):\n",
    "    def __init__(self, n_class=21):\n",
    "        super(FCN8s, self).__init__()\n",
    "        # VGG features\n",
    "        self.features = models.vgg16(pretrained=True).features\n",
    "\n",
    "        # fc6\n",
    "        self.fc6 = nn.Conv2d(512, 4096, 7)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.drop6 = nn.Dropout2d()\n",
    "\n",
    "        # fc7\n",
    "        self.fc7 = nn.Conv2d(4096, 4096, 1)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.drop7 = nn.Dropout2d()\n",
    "\n",
    "        #############\n",
    "        # CODE HERE #\n",
    "        #############\n",
    "\n",
    "        self._initialize_weights()\n",
    "        self.copy_params_from_vgg16()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        self.features[0].padding = (100,100)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.MaxPool2d):\n",
    "                m.ceil_mode=True\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                assert m.kernel_size[0] == m.kernel_size[1]\n",
    "                initial_weight = get_upsampling_weight(\n",
    "                    m.in_channels, m.out_channels, m.kernel_size[0])\n",
    "                m.weight.data.copy_(initial_weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #############\n",
    "        # CODE HERE #\n",
    "        #############\n",
    "\n",
    "        return x\n",
    "\n",
    "    def copy_params_from_vgg16(self):\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        for i, name in zip([0, 3], ['fc6', 'fc7']):\n",
    "            l1 = vgg16.classifier[i]\n",
    "            l2 = getattr(self, name)\n",
    "            l2.weight.data.copy_(l1.weight.data.view(l2.weight.size()))\n",
    "            l2.bias.data.copy_(l1.bias.data.view(l2.bias.size()))\n",
    "\n",
    "def get_upsampling_weight(in_channels, out_channels, kernel_size):\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = np.ogrid[:kernel_size, :kernel_size]\n",
    "    filt = (1 - abs(og[0] - center) / factor) * \\\n",
    "           (1 - abs(og[1] - center) / factor)\n",
    "    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size),\n",
    "                      dtype=np.float64)\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return torch.from_numpy(weight).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRpuF3DJvM0b"
   },
   "outputs": [],
   "source": [
    "model = FCN8s().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4YJ3_C_vM0c"
   },
   "source": [
    "**Data Loader functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2UXRqGDvM0c"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "def read_file(path_to_file):\n",
    "    with open(path_to_file) as f:\n",
    "        img_list = []\n",
    "        for line in f:\n",
    "            img_list.append(line[:-1])\n",
    "    return img_list\n",
    "\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def flip(I, flip_p):\n",
    "    if flip_p > 0.5:\n",
    "        return np.fliplr(I)\n",
    "    else:\n",
    "        return I\n",
    "\n",
    "def scale_im(img_temp, scale):\n",
    "    new_dims = (int(img_temp.shape[0] * scale), int(img_temp.shape[1] * scale))\n",
    "    return cv2.resize(img_temp, new_dims).astype(float)\n",
    "\n",
    "\n",
    "def get_data(chunk, gt_path='lab4/gt', img_path='lab4/img'):\n",
    "    assert len(chunk) == 1\n",
    "\n",
    "    scale = random.uniform(0.5, 1.3)\n",
    "    flip_p = random.uniform(0, 1)\n",
    "\n",
    "    images = cv2.imread(os.path.join(img_path, chunk[0] + '.jpg')).astype(float)\n",
    "\n",
    "    images = cv2.resize(images, (321, 321)).astype(float)\n",
    "    images = scale_im(images, scale)\n",
    "    images[:, :, 0] = images[:, :, 0] - 104.008\n",
    "    images[:, :, 1] = images[:, :, 1] - 116.669\n",
    "    images[:, :, 2] = images[:, :, 2] - 122.675\n",
    "    images = flip(images, flip_p)\n",
    "    images = images[:, :, :, np.newaxis]\n",
    "    images = images.transpose((3, 2, 0, 1))\n",
    "    images = torch.from_numpy(images.copy()).float()\n",
    "\n",
    "    gt = cv2.imread(os.path.join(gt_path, chunk[0] + '.png'))[:, :, 0]\n",
    "    gt[gt == 255] = 0\n",
    "    gt = flip(gt, flip_p)\n",
    "\n",
    "    dim = int(321 * scale)\n",
    "\n",
    "    gt = cv2.resize(gt, (dim, dim), interpolation=cv2.INTER_NEAREST).astype(float)\n",
    "\n",
    "    labels = gt[np.newaxis, :].copy()\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yu6bReWFvM0e"
   },
   "source": [
    "**Data load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOyK_Hx8vM0f"
   },
   "outputs": [],
   "source": [
    "img_list = read_file('lab4/list/train_aug.txt')\n",
    "data_list = []\n",
    "\n",
    "for i in range(10):\n",
    "    np.random.shuffle(img_list)\n",
    "    data_list.extend(img_list)\n",
    "data_gen = chunker(data_list, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EL9PWrZ-vM0h"
   },
   "outputs": [],
   "source": [
    "lr = # choose your lr\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "optimizer.zero_grad()\n",
    "max_iter = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSx1Mcu1vM0i"
   },
   "source": [
    "### 2.2 Write train code and print loss [2 points]\n",
    "- Use a cross-entropy loss.\n",
    "- Print a training loss for every 100 iterations.\n",
    "- Show that the training loss steadily decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-4Q_KqOvM0i"
   },
   "outputs": [],
   "source": [
    "def loss_calc(out, label):\n",
    "    #############\n",
    "    # CODE HERE #\n",
    "    #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBWSSkuuvM0k"
   },
   "outputs": [],
   "source": [
    "running_loss = 0.0\n",
    "\n",
    "for iter in range(max_iter + 1):\n",
    "    inputs, label = get_data(next(data_gen))\n",
    "\n",
    "    #############\n",
    "    # CODE HERE #\n",
    "    #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Game5WfAvM0l"
   },
   "source": [
    "### 2.3 Discuss the result [3 points]\n",
    "- Compare and Discuss the results. Note that it must contain the quantitative and qualitative results.\n",
    "- Use the below 'validation_miou' function to evaluate your model. Your model should perform better than the example model **(0.5 mIoU)**.\n",
    "- Use the above 'decode_label' function to visualize the network output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UybsasfQvM0m"
   },
   "outputs": [],
   "source": [
    "def validation_miou(model):\n",
    "    max_label = 20\n",
    "    hist = np.zeros((max_label + 1, max_label + 1))\n",
    "\n",
    "    def fast_hist(a, b, n):\n",
    "        k = (a >= 0) & (a < n)\n",
    "        return np.bincount(n * a[k].astype(int) + b[k], minlength=n ** 2).reshape(n, n)\n",
    "\n",
    "    val_list = open('lab4/list/val.txt').readlines()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, i in enumerate(val_list):\n",
    "            print('{}/{} ...'.format(idx + 1, len(val_list)))\n",
    "\n",
    "            img = cv2.imread(os.path.join('lab4/img', i[:-1] + '.jpg')).astype(float)\n",
    "\n",
    "            img[:, :, 0] -= 104.008\n",
    "            img[:, :, 1] -= 116.669\n",
    "            img[:, :, 2] -= 122.675\n",
    "\n",
    "            data = torch.from_numpy(img.transpose((2,0,1))).float().cuda().unsqueeze(0)\n",
    "            score = model(data)\n",
    "\n",
    "            output = score.cpu().data[0].numpy().transpose(1, 2, 0)\n",
    "            output = np.argmax(output, axis=2)\n",
    "            gt = cv2.imread(os.path.join('lab4/gt', i[:-1] + '.png'), 0)\n",
    "\n",
    "            hist += fast_hist(gt.flatten(), output.flatten(), max_label + 1)\n",
    "\n",
    "        miou = np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n",
    "        print(\"Mean iou = \", np.sum(miou) / len(miou))\n",
    "\n",
    "    return np.sum(miou) / len(miou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsHFMG6-vM0p"
   },
   "source": [
    "### *References*\n",
    "[1] FCN official code (https://github.com/shelhamer/fcn.berkeleyvision.org)\n",
    "\n",
    "[2] Upsampling method (https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)\n",
    "\n",
    "[3] Cs231n (http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
