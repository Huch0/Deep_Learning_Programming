{"cells":[{"cell_type":"markdown","metadata":{"id":"H21FIVCxWV-r"},"source":["#Lecture3.1: Neural network classifier"]},{"cell_type":"markdown","metadata":{"id":"ZRzjewOXWWC-"},"source":["## 1. About Neural Network"]},{"cell_type":"markdown","metadata":{"id":"hMNMch9QWWC_"},"source":["### 1.1 Linear Classifier Transition to Neural Networks\n","\n","#### Linear Classifier Review\n","\n","<!-- <img src=\"https://docs.google.com/uc?export=view&id=1CIFcScqBjjk_jTM3-4bwooqULYKhhiDa\" alt=\"no_image\" style=\"width: 900px;\"/> -->\n","<img src=\"./images/linear.png\" alt=\"linear\" style=\"width: 900px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"vich-tIlWWDI"},"source":["#### Linear Classifier Problems\n","- Can represent **linear** functions well\n","    - $ y = 2x + 3$\n","    - $ y = x_1 + x_2 $\n","    - $ y = x_1 + 3x_2 + 4x_3 $\n","- Cannot represent **non-linear** functions\n","    - $ y = 4x_1 + 2x_2^2 +3x_3^3 $\n","    - $ y = x_1x_2$\n","       \n","### 1.2 Introducing a Non-linear Function\n","\n","<!-- <img src=\"https://docs.google.com/uc?export=view&id=1rAaNxqxtTLzjqfzuZ0XoR8FTC7fT0r6r\" alt=\"no_image\" style=\"width: 900px;\"/> -->\n","<img src=\"./images/with_non_linear.png\" alt=\"with_non_linear\" style=\"width: 900px;\"/>\n","\n","\n","### Non-linear Function In-Depth\n","- Function: takes a number & perform mathematical operation\n","- Common Types of Non-linearity\n","    - ReLUs (Rectified Linear Units)      \n","    - Sigmoid     \n","    - Tanh\n","\n","#### Sigmoid (Logistic)\n","- $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n","- Input number $\\rightarrow$ [0, 1]\n","    - Large negative number $\\rightarrow$ 0\n","    - Large positive number $\\rightarrow$ 1\n","- Cons:\n","    1. Activation saturates at 0 or 1 with **gradients $\\approx$ 0**\n","        - No signal to update weights $\\rightarrow$ **cannot learn**\n","        - **Solution**: Have to carefully initialize weights to prevent this\n","    2. Outputs not centered around 0\n","        - If output always positive $\\rightarrow$ gradients always positive or negative $\\rightarrow$ **bad for gradient updates**\n","\n","#### Tanh\n","- $\\tanh(x) = 2 \\sigma(2x) -1$\n","    - A scaled sigmoid function\n","- Input number $\\rightarrow$ [-1, 1]\n","- Cons:\n","    1. Activation saturates at 0 or 1 with **gradients $\\approx$ 0**\n","        - No signal to update weights $\\rightarrow$ **cannot learn**\n","        - **Solution**: Have to carefully initialize weights to prevent this\n","\n","\n","#### ReLUs\n","- $f(x) = \\max(0, x)$\n","- Pros:\n","    1. Accelerates convergence $\\rightarrow$ **train faster**\n","    2. **Less computationally expensive operation** compared to Sigmoid/Tanh exponentials\n","- Cons:\n","    1. Many ReLU units \"die\" $\\rightarrow$ **gradients = 0** forever\n","        - **Solution**: careful learning rate choice\n","      \n"]},{"cell_type":"markdown","metadata":{"id":"DmW5-9WUWWDI"},"source":["## 2. Building a Neural Network with PyTorch\n","\n","### Model A: 1 Hidden Layer Neural Network (Sigmoid Activation)\n","<!-- <img src=\"https://docs.google.com/uc?export=view&id=1LMItCOS9LHqrAlsGP2a8qeGPjRAxZJ_q\" alt=\"no_image\" style=\"width: 900px;\"/> -->\n","<img src=\"./images/hidden.png\" alt=\"hidden\" style=\"width: 900px;\"/>\n","\n","\n","### Steps\n","- Step 1: Load Dataset\n","- Step 2: Make Dataset Iterable\n","- Step 3: Create Model Class\n","- Step 4: Instantiate Model Class\n","- Step 5: Instantiate Loss Class\n","- Step 6: Instantiate Optimizer Class\n","- Step 7: Train Model"]},{"cell_type":"markdown","metadata":{"id":"k0VDDfmCWWDJ"},"source":["### Step 1: Loading MNIST Train Dataset\n","**Images from 1 to 9**\n","\n","Similar to what we did in logistic regression, we will be using the same MNIST dataset where we load our training and testing datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LhhNZ0DrWWDK"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as dsets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QD5yoluWWDL"},"outputs":[],"source":["train_dataset = dsets.MNIST(root='./data/MNIST/',\n","                            train=True,\n","                            transform=transforms.ToTensor(),\n","                            download=True)\n","\n","test_dataset = dsets.MNIST(root='./data/MNIST/',\n","                           train=False,\n","                           transform=transforms.ToTensor())"]},{"cell_type":"markdown","metadata":{"id":"LryQVhbxWWDM"},"source":["### Step 2: Make Dataset Iterable"]},{"cell_type":"markdown","metadata":{"id":"khgO1Z_CWWDM"},"source":["**Batch sizes and iterations**\n","\n","Because we have 60000 training samples (images), we need to split them up to small groups (batches) and pass these batches of samples to our feedforward neural network subsesquently.\n","\n","There are a few reasons why we split them into batches. Passing your whole dataset as a single batch would:\n","\n","(1) require a lot of RAM/VRAM on your CPU/GPU and this might result in Out-of-Memory (OOM) errors.\n","\n","(2) cause unstable training if you just use all the errors accumulated in 60,000 images to update the model rather than gradually update the model. In layman terms, imagine you accumulated errors for a student taking an exam with 60,000 questions and punish the student all at the same time. It is much harder for the student to learn compared to letting the student learn it made mistakes and did well in smaller batches of questions like mini-tests!\n","\n","If we have 60,000 images and we want a batch size of 100, then we would have 600 iterations where each iteration involves passing 100 images to the model and getting their respective predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":592,"status":"ok","timestamp":1584682318803,"user":{"displayName":"­김도형(대학원/일반대학원 전기전자공학과)","photoUrl":"","userId":"05486823788267165662"},"user_tz":-540},"id":"beJpqZ2RWWDN","outputId":"4ae73e4f-942f-4a49-f40c-ba48846878d8"},"outputs":[{"data":{"text/plain":["600.0"]},"execution_count":99,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["60000 / 100"]},{"cell_type":"markdown","metadata":{"id":"7yymPf5aWWDO"},"source":["**Epochs**\n","\n","An epoch means that you have successfully passed the whole training set, 60,000 images, to the model. Continuing our example above, an epoch consists of 600 iterations.\n","\n","If we want to go through the whole dataset 5 times (5 epochs) for the model to learn, then we need 3000 iterations (600 x 5)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":625,"status":"ok","timestamp":1584682320494,"user":{"displayName":"­김도형(대학원/일반대학원 전기전자공학과)","photoUrl":"","userId":"05486823788267165662"},"user_tz":-540},"id":"RsTKW5bWWWDO","outputId":"a59f16fa-49ee-4bf8-a3c0-d11a6a687754"},"outputs":[{"data":{"text/plain":["3000"]},"execution_count":100,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["600 * 5"]},{"cell_type":"markdown","metadata":{"id":"Bh4HgjUTWWDQ"},"source":["**Bringing batch size, iterations and epochs together**\n","\n","As we have gone through above, we want to have 5 epochs, where each epoch would have 600 iterations and each iteration has a batch size of 100.\n","\n","Because we want 5 epochs, we need a total of 3000 iterations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2PsYiMOWWDS"},"outputs":[],"source":["batch_size = 100\n","n_iters = 3000\n","num_epochs = n_iters / (len(train_dataset) / batch_size)\n","num_epochs = int(num_epochs)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"CxCCYGHtWWDT"},"source":["### Step 3: Create Model Class"]},{"cell_type":"markdown","metadata":{"id":"eFcIf7u5WWDU"},"source":["**Creating our feedforward neural network**\n","\n","Compared to logistic regression with only a single linear layer, we know for an FNN we need an additional linear layer and non-linear layer.\n","\n","This translates to just 4 more lines of code!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r3f6CY0xWWDU"},"outputs":[],"source":["class FeedforwardNeuralNetModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(FeedforwardNeuralNetModel, self).__init__()\n","        # Linear function\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","\n","        # Non-linearity\n","        self.sigmoid = nn.Sigmoid()\n","\n","        # Linear function (readout)\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        # Linear function  # LINEAR\n","        out = self.fc1(x)\n","\n","        # Non-linearity  # NON-LINEAR\n","        out = self.sigmoid(out)\n","\n","        # Linear function (readout)  # LINEAR\n","        out = self.fc2(out)\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"Yyvz4tK8WWDW"},"source":["### Step 4: Instantiate Model Class\n","\n","- **Input** dimension: **784**\n","    - Size of image\n","    - $28 \\times 28 = 784$\n","- **Output** dimension: **10**\n","    - 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n","- **Hidden** dimension: **100**\n","    - Can be any number\n","    - Similar term\n","        - Number of neurons\n","        - Number of non-linear activation functions\n"]},{"cell_type":"markdown","metadata":{"id":"ZFn1RPcsWWDW"},"source":["**Instantiating our model class**\n","\n","Our input size is determined by the size of the image (numbers ranging from 0 to 9) which has a width of 28 pixels and a height of 28 pixels. Hence the size of our input is 784 (28 x 28).\n","\n","Our output size is what we are trying to predict. When we pass an image to our model, it will try to predict if it's 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9. That is a total of 10 classes, hence we have an output size of 10.\n","\n","Now the tricky part is in determining our hidden layer size, that is the size of our first linear layer prior to the non-linear layer. This can be any number, a larger number implies a bigger model with more parameters. Intuitively we think a bigger model equates to a better model, but a bigger model requires more training samples to learn and converge to a good model (also called curse of dimensionality). Hence, it is wise to pick the model size for the problem at hand. Because it is a simple problem of recognizing digits, we typically would not need a big model to achieve state-of-the-art results.\n","\n","On the flipside, too small of a hidden size would mean there would be insufficient model capacity to predict competently. In layman terms, too small of a capacity implies a smaller brain capacity so no matter how many training samples you give it, it has a maximum capacity in terms of its predictive power."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yw-g7gwJWWDW"},"outputs":[],"source":["input_dim = 28*28\n","hidden_dim = 100\n","output_dim = 10\n","\n","model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)"]},{"cell_type":"markdown","metadata":{"id":"FOoZ3_KVWWDY"},"source":["### Step 5: Instantiate Loss Class\n","\n","- Feedforward Neural Network: **Cross Entropy Loss**\n","    - _Logistic Regression_: **Cross Entropy Loss**\n","    - _Linear Regression_: **MSE**"]},{"cell_type":"markdown","metadata":{"id":"f3xcW2DvWWDY"},"source":["**Loss class**\n","\n","This is exactly the same as what we did in logistic regression. Because we are going through a classification problem, cross entropy function is required to compute the loss between our softmax outputs and our binary labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mtt_y1QLWWDY"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{"id":"px7GVkRhWWDZ"},"source":["### Step 6: Instantiate Optimizer Class\n","- Simplified equation\n","    - $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta$\n","        - $\\theta$: parameters (our tensors with gradient accumulation capabilities)\n","        - $\\eta$: learning rate (how fast we want to learn)\n","        - $\\nabla_\\theta$: parameters' gradients\n","- Even simplier equation\n","    - `parameters = parameters - learning_rate * parameters_gradients`\n","    - **At every iteration, we update our model's parameters**"]},{"cell_type":"markdown","metadata":{"id":"0ZMMT9prWWDa"},"source":["**Optimizer class**\n","\n","Learning rate determines how fast the algorithm learns. Too small and the algorithm learns too slowly, too large and the algorithm learns too fast resulting in instabilities.\n","\n","Intuitively, we would think a larger learning rate would be better because we learn faster. But that's not true. Imagine we pass 10 images to a human to learn how to recognize whether the image is a hot dog or not, and it got half right and half wrong.\n","\n","A well defined learning rate (neither too small or large) is equivalent to rewarding the human with a sweet for getting the first half right, and punishing the other half the human got wrong with a smack on the palm.\n","\n","A large learning rate would be equivalent to feeding a thousand sweets to the human and smacking a thousand times on the human's palm. This would lead in a very unstable learning environment. Similarly, we will observe that the algorithm's convergence path will be extremely unstable if you use a large learning rate without reducing it subsequently.\n","\n","We are using an optimization algorithm called Stochastic Gradient Descent (SGD) which is essentially what we covered above on calculating the parameters' gradients multiplied by the learning rate then using it to update our parameters gradually. There's an in-depth analysis of various optimization algorithms on top of SGD in another section."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lwjfSlUkWWDa"},"outputs":[],"source":["learning_rate = 0.1\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","metadata":{"id":"A3fJTmfmWWDb"},"source":["### Parameters In-Depth"]},{"cell_type":"markdown","metadata":{"id":"x8NPApjrWWDb"},"source":["**Linear layers' parameters**\n","\n","In a simple linear layer it's $Y = AX + B$, and our parameters are $A$ and bias $B$.\n","    \n","Hence, each linear layer would have 2 groups of parameters  $A$ and $B$. It is critical to take note that our non-linear layers have no parameters to update. They are merely mathematical functions performed on $Y$, the output of our linear layers.\n","    \n","This would return a Python generator object, so you need to call list on the generator object to access anything meaningful."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":869,"status":"ok","timestamp":1584682329264,"user":{"displayName":"­김도형(대학원/일반대학원 전기전자공학과)","photoUrl":"","userId":"05486823788267165662"},"user_tz":-540},"id":"uHBjjqZJWWDc","outputId":"23f88600-7b1c-409a-8352-c68ec1ba3ff9"},"outputs":[{"name":"stdout","output_type":"stream","text":["<generator object Module.parameters at 0x7f0117a3a8e0>\n"]}],"source":["print(model.parameters())"]},{"cell_type":"markdown","metadata":{"id":"RwHuv_etWWDd"},"source":["Here we call list on the generator object and getting the length of the list. This would return 4 because we've 2 linear layers, and each layer has 2 groups of parameters $A$ and $b$."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":716,"status":"ok","timestamp":1584682330155,"user":{"displayName":"­김도형(대학원/일반대학원 전기전자공학과)","photoUrl":"","userId":"05486823788267165662"},"user_tz":-540},"id":"78kJAsHPWWDd","outputId":"45a9fba0-3ded-4dc4-acb8-1c02ef545cec"},"outputs":[{"name":"stdout","output_type":"stream","text":["4\n"]}],"source":["print(len(list(model.parameters())))"]},{"cell_type":"markdown","metadata":{"id":"RSFFKhsvWWDg"},"source":["Our first linear layer parameters, $A_1$, would be of size 100 x 784. This is because we've an input size of 784 (28 x 28) and a hidden size of 100."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":412,"status":"ok","timestamp":1584682331136,"user":{"displayName":"­김도형(대학원/일반대학원 전기전자공학과)","photoUrl":"","userId":"05486823788267165662"},"user_tz":-540},"id":"e4mxU_ReWWDg","outputId":"a4958013-3b66-4d71-a640-a8a836d41575"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([100, 784])\n"]}],"source":["# FC 1 Parameters\n","print(list(model.parameters())[0].size())"]},{"cell_type":"markdown","metadata":{"id":"GMCPyKtcWWDh"},"source":["Our first linear layer bias parameters, $B_1$, would be of size 100 which is our hidden size."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":561,"status":"ok","timestamp":1584682332527,"user":{"displayName":"­김도형(대학원/일반대학원 전기전자공학과)","photoUrl":"","userId":"05486823788267165662"},"user_tz":-540},"id":"AVWDQWVGWWDi","outputId":"2e9d055f-2a85-4d25-ab74-8358e0759210"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([100])\n"]}],"source":["# FC 1 Bias Parameters\n","print(list(model.parameters())[1].size())"]},{"cell_type":"markdown","metadata":{"id":"Q6Odqu-1WWDj"},"source":["Our second linear layer is our readout layer, where the parameters $A_2$ would be of size 10 x 100. This is because our output size is 10 and hidden size is 100."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":682,"status":"ok","timestamp":1584682333915,"user":{"displayName":"­김도형(대학원/일반대학원 전기전자공학과)","photoUrl":"","userId":"05486823788267165662"},"user_tz":-540},"id":"XrdPX0_uWWDj","outputId":"05f0c2af-be4a-4f30-be0e-e8d1cb6e981b"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([10, 100])\n"]}],"source":["# FC 2 Parameters\n","print(list(model.parameters())[2].size())"]},{"cell_type":"markdown","metadata":{"id":"eDEtBaWbWWDl"},"source":["Likewise our readout layer's bias $B_1$ would just be 10, the size of our output."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":482,"status":"ok","timestamp":1584682334877,"user":{"displayName":"­김도형(대학원/일반대학원 전기전자공학과)","photoUrl":"","userId":"05486823788267165662"},"user_tz":-540},"id":"BdUyvjqfWWDl","outputId":"3072b75c-25f5-412b-f610-cd7c5458c98e"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([10])\n"]}],"source":["# FC 2 Bias Parameters\n","print(list(model.parameters())[3].size())"]},{"cell_type":"markdown","metadata":{"id":"7_guQsTxWWDm"},"source":["The diagram below shows the interaction amongst our input $X$ and our linear layers' parameters $A_1$, $B_1$, $A_2$, and $B_2$ to reach to the final size of 10 x 1."]},{"cell_type":"markdown","metadata":{"id":"m1_-xkbTWWDn"},"source":["<!-- <img src=\"https://docs.google.com/uc?export=view&id=1WpJkXR6-3rZ3r4WaqZEBwqcAd5Wg0yGJ\" alt=\"no_image\" style=\"width: 900px;\"/> -->\n","<img src=\"./images/calculation.png\" alt=\"calculation\" style=\"width: 900px;\"/>"]},{"cell_type":"markdown","metadata":{"id":"9qFfv343WWDn"},"source":["### Step 7: Train Model\n","- Process\n","    1. Convert inputs to tensors with gradient accumulation capabilities\n","    2. Clear gradient buffers\n","    3. Get output given inputs\n","    4. Get loss\n","    5. Get gradients w.r.t. parameters\n","    6. Update parameters using gradients\n","        - `parameters = parameters - learning_rate * parameters_gradients`\n","    7. REPEAT"]},{"cell_type":"markdown","metadata":{"id":"2MUhs9zXWWDn"},"source":["**7-step training process**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"elapsed":30557,"status":"ok","timestamp":1584682367014,"user":{"displayName":"­김도형(대학원/일반대학원 전기전자공학과)","photoUrl":"","userId":"05486823788267165662"},"user_tz":-540},"id":"G8YYhbWcWWDo","outputId":"2a6cd6b5-e40b-4aee-a916-9e3607581fae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration: 500. Loss: 0.5384832620620728. Accuracy: 85.98\n","Iteration: 1000. Loss: 0.3519541621208191. Accuracy: 89.73\n","Iteration: 1500. Loss: 0.2670428454875946. Accuracy: 90.49\n","Iteration: 2000. Loss: 0.39770033955574036. Accuracy: 91.1\n","Iteration: 2500. Loss: 0.2081325352191925. Accuracy: 91.72\n","Iteration: 3000. Loss: 0.37106427550315857. Accuracy: 92.06\n"]}],"source":["iter = 0\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        # Load images with gradient accumulation capabilities\n","        images = images.view(-1, 28*28)\n","\n","        # Clear gradients w.r.t. parameters\n","        optimizer.zero_grad()\n","\n","        # Forward pass to get output/logits\n","        outputs = model(images)\n","\n","        # Calculate Loss: softmax --> cross entropy loss\n","        loss = criterion(outputs, labels)\n","\n","        # Getting gradients w.r.t. parameters\n","        loss.backward()\n","\n","        # Updating parameters\n","        optimizer.step()\n","\n","        iter += 1\n","\n","        if iter % 500 == 0:\n","            # Calculate Accuracy\n","            correct = 0\n","            total = 0\n","            # Iterate through test dataset\n","            for images, labels in test_loader:\n","                # Load images with gradient accumulation capabilities\n","                images = images.view(-1, 28*28)\n","\n","                # Forward pass only to get logits/output\n","                outputs = model(images)\n","\n","                # Get predictions from the maximum value\n","                _, predicted = torch.max(outputs.data, 1)\n","\n","                # Total number of labels\n","                total += labels.size(0)\n","\n","                # Total correct predictions\n","                correct += (predicted == labels).sum()\n","\n","            accuracy = 100 * correct.item() / total\n","\n","            # Print Loss\n","            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"]},{"cell_type":"markdown","metadata":{"id":"X1bp7zpuWWDp"},"source":["### Model B: 1 Hidden Layer Feedforward Neural Network (Tanh Activation)\n","<!-- <img src=\"https://docs.google.com/uc?export=view&id=1LMItCOS9LHqrAlsGP2a8qeGPjRAxZJ_q\" alt=\"no_image\" style=\"width: 900px;\"/> -->\n","<img src=\"./images/hidden.png\" alt=\"hidden\" style=\"width: 900px;\"/>\n","\n","### Steps\n","- Step 1: Load Dataset\n","- Step 2: Make Dataset Iterable\n","- **Step 3: Create Model Class**\n","- Step 4: Instantiate Model Class\n","- Step 5: Instantiate Loss Class\n","- Step 6: Instantiate Optimizer Class\n","- Step 7: Train Model"]},{"cell_type":"markdown","metadata":{"id":"psCvqWPYWWDr"},"source":["**1-layer FNN with Tanh Activation**\n","\n","The only difference here compared to previously is that we are using Tanh activation instead of Sigmoid activation. This affects step 3."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"elapsed":31394,"status":"ok","timestamp":1584682399293,"user":{"displayName":"­김도형(대학원/일반대학원 전기전자공학과)","photoUrl":"","userId":"05486823788267165662"},"user_tz":-540},"id":"i_L_f5v0WWDr","outputId":"55cf5537-486b-4478-d2c9-7c7cda063dfa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration: 500. Loss: 0.3234499394893646. Accuracy: 91.34\n","Iteration: 1000. Loss: 0.24714773893356323. Accuracy: 92.5\n","Iteration: 1500. Loss: 0.1349847912788391. Accuracy: 93.37\n","Iteration: 2000. Loss: 0.20334944128990173. Accuracy: 93.96\n","Iteration: 2500. Loss: 0.13155107200145721. Accuracy: 94.61\n","Iteration: 3000. Loss: 0.09727134555578232. Accuracy: 95.13\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as dsets\n","\n","'''\n","STEP 1: LOADING DATASET\n","'''\n","\n","train_dataset = dsets.MNIST(root='./data/MNIST/',\n","                            train=True,\n","                            transform=transforms.ToTensor(),\n","                            download=True)\n","\n","test_dataset = dsets.MNIST(root='./data/MNIST/',\n","                           train=False,\n","                           transform=transforms.ToTensor())\n","\n","'''\n","STEP 2: MAKING DATASET ITERABLE\n","'''\n","\n","batch_size = 100\n","n_iters = 3000\n","num_epochs = n_iters / (len(train_dataset) / batch_size)\n","num_epochs = int(num_epochs)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)\n","\n","'''\n","STEP 3: CREATE MODEL CLASS\n","'''\n","class FeedforwardNeuralNetModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(FeedforwardNeuralNetModel, self).__init__()\n","        # Linear function\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        # Non-linearity\n","        self.tanh = nn.Tanh()\n","        # Linear function (readout)\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        # Linear function\n","        out = self.fc1(x)\n","        # Non-linearity\n","        out = self.tanh(out)\n","        # Linear function (readout)\n","        out = self.fc2(out)\n","        return out\n","'''\n","STEP 4: INSTANTIATE MODEL CLASS\n","'''\n","input_dim = 28*28\n","hidden_dim = 100\n","output_dim = 10\n","\n","model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n","\n","'''\n","STEP 5: INSTANTIATE LOSS CLASS\n","'''\n","criterion = nn.CrossEntropyLoss()\n","\n","'''\n","STEP 6: INSTANTIATE OPTIMIZER CLASS\n","'''\n","learning_rate = 0.1\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","'''\n","STEP 7: TRAIN THE MODEL\n","'''\n","iter = 0\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        # Load images with gradient accumulation capabilities\n","        images = images.view(-1, 28*28)\n","\n","        # Clear gradients w.r.t. parameters\n","        optimizer.zero_grad()\n","\n","        # Forward pass to get output/logits\n","        outputs = model(images)\n","\n","        # Calculate Loss: softmax --> cross entropy loss\n","        loss = criterion(outputs, labels)\n","\n","        # Getting gradients w.r.t. parameters\n","        loss.backward()\n","\n","        # Updating parameters\n","        optimizer.step()\n","\n","        iter += 1\n","\n","        if iter % 500 == 0:\n","            # Calculate Accuracy\n","            correct = 0\n","            total = 0\n","            # Iterate through test dataset\n","            for images, labels in test_loader:\n","                # Load images with gradient accumulation capabilities\n","                images = images.view(-1, 28*28)\n","\n","                # Forward pass only to get logits/output\n","                outputs = model(images)\n","\n","                # Get predictions from the maximum value\n","                _, predicted = torch.max(outputs.data, 1)\n","\n","                # Total number of labels\n","                total += labels.size(0)\n","\n","                # Total correct predictions\n","                correct += (predicted == labels).sum()\n","\n","            accuracy = 100 * correct.item() / total\n","\n","            # Print Loss\n","            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"]},{"cell_type":"markdown","metadata":{"id":"RxdUJ9OjWWDt"},"source":["### Model C: 1 Hidden Layer Feedforward Neural Network (ReLU Activation)\n","<!-- <img src=\"https://docs.google.com/uc?export=view&id=1LMItCOS9LHqrAlsGP2a8qeGPjRAxZJ_q\" alt=\"no_image\" style=\"width: 900px;\"/> -->\n","<img src=\"./images/hidden.png\" alt=\"hidden\" style=\"width: 900px;\"/>\n","\n","### Steps\n","- Step 1: Load Dataset\n","- Step 2: Make Dataset Iterable\n","- **Step 3: Create Model Class**\n","- Step 4: Instantiate Model Class\n","- Step 5: Instantiate Loss Class\n","- Step 6: Instantiate Optimizer Class\n","- Step 7: Train Model"]},{"cell_type":"markdown","metadata":{"id":"degqCsyqWWDu"},"source":["**1-layer FNN with ReLU Activation**\n","\n","The only difference again is in using ReLU activation and it affects step 3."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"elapsed":57646,"status":"ok","timestamp":1584682429348,"user":{"displayName":"­김도형(대학원/일반대학원 전기전자공학과)","photoUrl":"","userId":"05486823788267165662"},"user_tz":-540},"id":"zgqVzDt2WWDu","outputId":"7e00e440-f3ab-4f3c-f576-c73870d6fb70"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration: 500. Loss: 0.2303968071937561. Accuracy: 91.31\n","Iteration: 1000. Loss: 0.336838960647583. Accuracy: 92.74\n","Iteration: 1500. Loss: 0.1881215125322342. Accuracy: 94.02\n","Iteration: 2000. Loss: 0.12567868828773499. Accuracy: 94.83\n","Iteration: 2500. Loss: 0.21342559158802032. Accuracy: 95.46\n","Iteration: 3000. Loss: 0.09440813213586807. Accuracy: 95.95\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as dsets\n","\n","'''\n","STEP 1: LOADING DATASET\n","'''\n","\n","train_dataset = dsets.MNIST(root='./data/MNIST/',\n","                            train=True,\n","                            transform=transforms.ToTensor(),\n","                            download=True)\n","\n","test_dataset = dsets.MNIST(root='./data/MNIST/',\n","                           train=False,\n","                           transform=transforms.ToTensor())\n","\n","'''\n","STEP 2: MAKING DATASET ITERABLE\n","'''\n","\n","batch_size = 100\n","n_iters = 3000\n","num_epochs = n_iters / (len(train_dataset) / batch_size)\n","num_epochs = int(num_epochs)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)\n","\n","'''\n","STEP 3: CREATE MODEL CLASS\n","'''\n","class FeedforwardNeuralNetModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(FeedforwardNeuralNetModel, self).__init__()\n","        # Linear function\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        # Non-linearity\n","        self.relu = nn.ReLU()\n","        # Linear function (readout)\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        # Linear function\n","        out = self.fc1(x)\n","        # Non-linearity\n","        out = self.relu(out)\n","        # Linear function (readout)\n","        out = self.fc2(out)\n","        return out\n","'''\n","STEP 4: INSTANTIATE MODEL CLASS\n","'''\n","input_dim = 28*28\n","hidden_dim = 100\n","output_dim = 10\n","\n","model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n","\n","'''\n","STEP 5: INSTANTIATE LOSS CLASS\n","'''\n","criterion = nn.CrossEntropyLoss()\n","\n","\n","'''\n","STEP 6: INSTANTIATE OPTIMIZER CLASS\n","'''\n","learning_rate = 0.1\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","'''\n","STEP 7: TRAIN THE MODEL\n","'''\n","iter = 0\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        # Load images with gradient accumulation capabilities\n","        images = images.view(-1, 28*28)\n","\n","        # Clear gradients w.r.t. parameters\n","        optimizer.zero_grad()\n","\n","        # Forward pass to get output/logits\n","        outputs = model(images)\n","\n","        # Calculate Loss: softmax --> cross entropy loss\n","        loss = criterion(outputs, labels)\n","\n","        # Getting gradients w.r.t. parameters\n","        loss.backward()\n","\n","        # Updating parameters\n","        optimizer.step()\n","\n","        iter += 1\n","\n","        if iter % 500 == 0:\n","            # Calculate Accuracy\n","            correct = 0\n","            total = 0\n","            # Iterate through test dataset\n","            for images, labels in test_loader:\n","                # Load images with gradient accumulation capabilities\n","                images = images.view(-1, 28*28)\n","\n","                # Forward pass only to get logits/output\n","                outputs = model(images)\n","\n","                # Get predictions from the maximum value\n","                _, predicted = torch.max(outputs.data, 1)\n","\n","                # Total number of labels\n","                total += labels.size(0)\n","\n","                # Total correct predictions\n","                correct += (predicted == labels).sum()\n","\n","            accuracy = 100 * correct.item() / total\n","\n","            # Print Loss\n","            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"]},{"cell_type":"markdown","metadata":{"id":"D0ETALQLWWDw"},"source":["### Model D: 2 Hidden Layer Feedforward Neural Network (ReLU Activation)\n","<!-- <img src=\"https://docs.google.com/uc?export=view&id=1_l4RyBIpmbsWvXHB1PMv0r1JtvNiA1vm\" alt=\"no_image\" style=\"width: 900px;\"/> -->\n","<img src=\"./images/hidden2.png\" alt=\"hidden2\" style=\"width: 900px;\"/>\n","\n","### Steps\n","- Step 1: Load Dataset\n","- Step 2: Make Dataset Iterable\n","- **Step 3: Create Model Class**\n","- Step 4: Instantiate Model Class\n","- Step 5: Instantiate Loss Class\n","- Step 6: Instantiate Optimizer Class\n","- Step 7: Train Model"]},{"cell_type":"markdown","metadata":{"id":"6sbKZxfjWWDx"},"source":["**2-layer FNN with ReLU Activation**\n","\n","This is a bigger difference that increases your model's capacity by adding another linear layer and non-linear layer which affects step 3."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"elapsed":62265,"status":"ok","timestamp":1584682460675,"user":{"displayName":"­김도형(대학원/일반대학원 전기전자공학과)","photoUrl":"","userId":"05486823788267165662"},"user_tz":-540},"id":"IsK1qXFjWWDx","outputId":"7901e260-d711-441a-e68e-548974674234"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration: 500. Loss: 0.24625259637832642. Accuracy: 90.78\n","Iteration: 1000. Loss: 0.14038103818893433. Accuracy: 93.58\n","Iteration: 1500. Loss: 0.13254402577877045. Accuracy: 94.69\n","Iteration: 2000. Loss: 0.15604619681835175. Accuracy: 95.61\n","Iteration: 2500. Loss: 0.14129334688186646. Accuracy: 95.87\n","Iteration: 3000. Loss: 0.07314524054527283. Accuracy: 96.58\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as dsets\n","\n","'''\n","STEP 1: LOADING DATASET\n","'''\n","\n","train_dataset = dsets.MNIST(root='./data/MNIST/',\n","                            train=True,\n","                            transform=transforms.ToTensor(),\n","                            download=True)\n","\n","test_dataset = dsets.MNIST(root='./data/MNIST/',\n","                           train=False,\n","                           transform=transforms.ToTensor())\n","\n","'''\n","STEP 2: MAKING DATASET ITERABLE\n","'''\n","\n","batch_size = 100\n","n_iters = 3000\n","num_epochs = n_iters / (len(train_dataset) / batch_size)\n","num_epochs = int(num_epochs)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)\n","\n","'''\n","STEP 3: CREATE MODEL CLASS\n","'''\n","class FeedforwardNeuralNetModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(FeedforwardNeuralNetModel, self).__init__()\n","        # Linear function 1: 784 --> 100\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        # Non-linearity 1\n","        self.relu1 = nn.ReLU()\n","\n","        # Linear function 2: 100 --> 100\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        # Non-linearity 2\n","        self.relu2 = nn.ReLU()\n","\n","        # Linear function 3 (readout): 100 --> 10\n","        self.fc3 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        # Linear function 1\n","        out = self.fc1(x)\n","        # Non-linearity 1\n","        out = self.relu1(out)\n","\n","        # Linear function 2\n","        out = self.fc2(out)\n","        # Non-linearity 2\n","        out = self.relu2(out)\n","\n","        # Linear function 3 (readout)\n","        out = self.fc3(out)\n","        return out\n","'''\n","STEP 4: INSTANTIATE MODEL CLASS\n","'''\n","input_dim = 28*28\n","hidden_dim = 100\n","output_dim = 10\n","\n","model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n","\n","'''\n","STEP 5: INSTANTIATE LOSS CLASS\n","'''\n","criterion = nn.CrossEntropyLoss()\n","\n","\n","'''\n","STEP 6: INSTANTIATE OPTIMIZER CLASS\n","'''\n","learning_rate = 0.1\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","'''\n","STEP 7: TRAIN THE MODEL\n","'''\n","iter = 0\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        # Load images with gradient accumulation capabilities\n","        images = images.view(-1, 28*28)\n","        labels = labels\n","\n","        # Clear gradients w.r.t. parameters\n","        optimizer.zero_grad()\n","\n","        # Forward pass to get output/logits\n","        outputs = model(images)\n","\n","        # Calculate Loss: softmax --> cross entropy loss\n","        loss = criterion(outputs, labels)\n","\n","        # Getting gradients w.r.t. parameters\n","        loss.backward()\n","\n","        # Updating parameters\n","        optimizer.step()\n","\n","        iter += 1\n","\n","        if iter % 500 == 0:\n","            # Calculate Accuracy\n","            correct = 0\n","            total = 0\n","            # Iterate through test dataset\n","            for images, labels in test_loader:\n","                # Load images with gradient accumulation capabilities\n","                images = images.view(-1, 28*28)\n","\n","                # Forward pass only to get logits/output\n","                outputs = model(images)\n","\n","                # Get predictions from the maximum value\n","                _, predicted = torch.max(outputs.data, 1)\n","\n","                # Total number of labels\n","                total += labels.size(0)\n","\n","                # Total correct predictions\n","                correct += (predicted == labels).sum()\n","\n","            accuracy = 100 * correct.item() / total\n","\n","            # Print Loss\n","            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"]},{"cell_type":"markdown","metadata":{"id":"dC-w6lRhWWDy"},"source":["### Model E: 3 Hidden Layer Feedforward Neural Network (ReLU Activation)\n","<!-- <img src=\"https://docs.google.com/uc?export=view&id=1yPATqCgtdRcGH2kvT3HbCajE4Brh7kub\" alt=\"no_image\" style=\"width: 900px;\"/> -->\n","<img src=\"./images/hidden3.png\" alt=\"hidden3\" style=\"width: 900px;\"/>\n","\n","### Steps\n","- Step 1: Load Dataset\n","- Step 2: Make Dataset Iterable\n","- **Step 3: Create Model Class**\n","- Step 4: Instantiate Model Class\n","- Step 5: Instantiate Loss Class\n","- Step 6: Instantiate Optimizer Class\n","- Step 7: Train Model"]},{"cell_type":"markdown","metadata":{"id":"6sCElDk5WWDz"},"source":["**3-layer FNN with ReLU Activation**\n","\n","Let's add one more layer! Bigger model capacity. But will it be better? Remember what we talked about on curse of dimensionality?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"elapsed":93868,"status":"ok","timestamp":1584682492984,"user":{"displayName":"­김도형(대학원/일반대학원 전기전자공학과)","photoUrl":"","userId":"05486823788267165662"},"user_tz":-540},"id":"vWjKLqiCWWDz","outputId":"3849f6d6-378d-4a0a-a6c1-b15219dcb0e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration: 500. Loss: 0.5468496680259705. Accuracy: 90.18\n","Iteration: 1000. Loss: 0.16326414048671722. Accuracy: 93.99\n","Iteration: 1500. Loss: 0.26454392075538635. Accuracy: 95.24\n","Iteration: 2000. Loss: 0.1020004004240036. Accuracy: 95.92\n","Iteration: 2500. Loss: 0.0668027475476265. Accuracy: 96.67\n","Iteration: 3000. Loss: 0.15142413973808289. Accuracy: 96.22\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as dsets\n","\n","'''\n","STEP 1: LOADING DATASET\n","'''\n","\n","train_dataset = dsets.MNIST(root='./data/MNIST/',\n","                            train=True,\n","                            transform=transforms.ToTensor(),\n","                            download=True)\n","\n","test_dataset = dsets.MNIST(root='./data/MNIST/',\n","                           train=False,\n","                           transform=transforms.ToTensor())\n","\n","'''\n","STEP 2: MAKING DATASET ITERABLE\n","'''\n","\n","batch_size = 100\n","n_iters = 3000\n","num_epochs = n_iters / (len(train_dataset) / batch_size)\n","num_epochs = int(num_epochs)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)\n","\n","'''\n","STEP 3: CREATE MODEL CLASS\n","'''\n","class FeedforwardNeuralNetModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(FeedforwardNeuralNetModel, self).__init__()\n","        # Linear function 1: 784 --> 100\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        # Non-linearity 1\n","        self.relu1 = nn.ReLU()\n","\n","        # Linear function 2: 100 --> 100\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        # Non-linearity 2\n","        self.relu2 = nn.ReLU()\n","\n","        # Linear function 3: 100 --> 100\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        # Non-linearity 3\n","        self.relu3 = nn.ReLU()\n","\n","        # Linear function 4 (readout): 100 --> 10\n","        self.fc4 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        # Linear function 1\n","        out = self.fc1(x)\n","        # Non-linearity 1\n","        out = self.relu1(out)\n","\n","        # Linear function 2\n","        out = self.fc2(out)\n","        # Non-linearity 2\n","        out = self.relu2(out)\n","\n","        # Linear function 2\n","        out = self.fc3(out)\n","        # Non-linearity 2\n","        out = self.relu3(out)\n","\n","        # Linear function 4 (readout)\n","        out = self.fc4(out)\n","        return out\n","'''\n","STEP 4: INSTANTIATE MODEL CLASS\n","'''\n","input_dim = 28*28\n","hidden_dim = 100\n","output_dim = 10\n","\n","model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n","\n","'''\n","STEP 5: INSTANTIATE LOSS CLASS\n","'''\n","criterion = nn.CrossEntropyLoss()\n","\n","\n","'''\n","STEP 6: INSTANTIATE OPTIMIZER CLASS\n","'''\n","learning_rate = 0.1\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","'''\n","STEP 7: TRAIN THE MODEL\n","'''\n","iter = 0\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        # Load images with gradient accumulation capabilities\n","        images = images.view(-1, 28*28)\n","\n","        # Clear gradients w.r.t. parameters\n","        optimizer.zero_grad()\n","\n","        # Forward pass to get output/logits\n","        outputs = model(images)\n","\n","        # Calculate Loss: softmax --> cross entropy loss\n","        loss = criterion(outputs, labels)\n","\n","        # Getting gradients w.r.t. parameters\n","        loss.backward()\n","\n","        # Updating parameters\n","        optimizer.step()\n","\n","        iter += 1\n","\n","        if iter % 500 == 0:\n","            # Calculate Accuracy\n","            correct = 0\n","            total = 0\n","            # Iterate through test dataset\n","            for images, labels in test_loader:\n","                # Load images with gradient accumulation capabilities\n","                images = images.view(-1, 28*28)\n","\n","                # Forward pass only to get logits/output\n","                outputs = model(images)\n","\n","                # Get predictions from the maximum value\n","                _, predicted = torch.max(outputs.data, 1)\n","\n","                # Total number of labels\n","                total += labels.size(0)\n","\n","                # Total correct predictions\n","                correct += (predicted == labels).sum()\n","\n","            accuracy = 100 * correct.item() / total\n","\n","            # Print Loss\n","            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"]},{"cell_type":"markdown","metadata":{"id":"oNYiYAFPWWD0"},"source":["### General Comments on FNNs\n","- 2 ways to expand a neural network\n","    - More non-linear activation units (neurons)\n","    - More hidden layers\n","- Cons\n","    - Need a larger dataset\n","        - Curse of dimensionality\n","    - Does not necessarily mean higher accuracy"]},{"cell_type":"markdown","metadata":{"id":"hmfVJcEIWWD0"},"source":["## 3. Building a Feedforward Neural Network with PyTorch (GPU)"]},{"cell_type":"markdown","metadata":{"id":"Krc9sopKWWD1"},"source":["<!-- <img src=\"https://docs.google.com/uc?export=view&id=1yPATqCgtdRcGH2kvT3HbCajE4Brh7kub\" alt=\"no_image\" style=\"width: 900px;\"/> -->\n","<img src=\"./images/hidden3.png\" alt=\"hidden3\" style=\"width: 900px;\" />\n","\n","GPU: 2 things must be on GPU\n","- `model`\n","- `tensors with gradient accumulation capabilities`\n","\n","### Steps\n","- Step 1: Load Dataset\n","- Step 2: Make Dataset Iterable\n","- Step 3: Create Model Class\n","- **Step 4: Instantiate Model Class**\n","- Step 5: Instantiate Loss Class\n","- Step 6: Instantiate Optimizer Class\n","- **Step 7: Train Model**"]},{"cell_type":"markdown","metadata":{"id":"xAgoEBtaWWD1"},"source":["**3-layer FNN with ReLU Activation on GPU**\n","\n","Only step 4 and 7 of the CPU code will be affected and it's a simple change."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83647,"status":"ok","timestamp":1725941260587,"user":{"displayName":"전상률","userId":"01419787166512794662"},"user_tz":-540},"id":"7aqdNaqYWWD1","outputId":"5ff99fc9-f1bd-404d-9ad7-2b91cf119c66"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 10511485.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 28881/28881 [00:00<00:00, 376982.21it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 3205796.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Failed to download (trying next):\n","HTTP Error 403: Forbidden\n","\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4542/4542 [00:00<00:00, 4169518.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw\n","\n","Iteration: 500. Loss: 0.34025245904922485. Accuracy: 90.25\n","Iteration: 1000. Loss: 0.22724656760692596. Accuracy: 94.28\n","Iteration: 1500. Loss: 0.2041848599910736. Accuracy: 95.38\n","Iteration: 2000. Loss: 0.17993171513080597. Accuracy: 95.42\n","Iteration: 2500. Loss: 0.044235263019800186. Accuracy: 96.55\n","Iteration: 3000. Loss: 0.08930499106645584. Accuracy: 96.96\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as dsets\n","\n","'''\n","STEP 1: LOADING DATASET\n","'''\n","\n","train_dataset = dsets.MNIST(root='./data/MNIST/',\n","                            train=True,\n","                            transform=transforms.ToTensor(),\n","                            download=True)\n","\n","test_dataset = dsets.MNIST(root='./data/MNIST/',\n","                           train=False,\n","                           transform=transforms.ToTensor())\n","\n","'''\n","STEP 2: MAKING DATASET ITERABLE\n","'''\n","\n","batch_size = 100\n","n_iters = 3000\n","num_epochs = n_iters / (len(train_dataset) / batch_size)\n","num_epochs = int(num_epochs)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False)\n","\n","'''\n","STEP 3: CREATE MODEL CLASS\n","'''\n","class FeedforwardNeuralNetModel(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(FeedforwardNeuralNetModel, self).__init__()\n","        # Linear function 1: 784 --> 100\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        # Non-linearity 1\n","        self.relu1 = nn.ReLU()\n","\n","        # Linear function 2: 100 --> 100\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        # Non-linearity 2\n","        self.relu2 = nn.ReLU()\n","\n","        # Linear function 3: 100 --> 100\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        # Non-linearity 3\n","        self.relu3 = nn.ReLU()\n","\n","        # Linear function 4 (readout): 100 --> 10\n","        self.fc4 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        # Linear function 1\n","        out = self.fc1(x)\n","        # Non-linearity 1\n","        out = self.relu1(out)\n","\n","        # Linear function 2\n","        out = self.fc2(out)\n","        # Non-linearity 2\n","        out = self.relu2(out)\n","\n","        # Linear function 2\n","        out = self.fc3(out)\n","        # Non-linearity 2\n","        out = self.relu3(out)\n","\n","        # Linear function 4 (readout)\n","        out = self.fc4(out)\n","        return out\n","'''\n","STEP 4: INSTANTIATE MODEL CLASS\n","'''\n","input_dim = 28*28\n","hidden_dim = 100\n","output_dim = 10\n","\n","model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n","\n","#######################\n","#  USE GPU FOR MODEL  #\n","#######################\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","'''\n","STEP 5: INSTANTIATE LOSS CLASS\n","'''\n","criterion = nn.CrossEntropyLoss()\n","\n","\n","'''\n","STEP 6: INSTANTIATE OPTIMIZER CLASS\n","'''\n","learning_rate = 0.1\n","\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","'''\n","STEP 7: TRAIN THE MODEL\n","'''\n","iter = 0\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","\n","        #######################\n","        #  USE GPU FOR MODEL  #\n","        #######################\n","        images = images.view(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","\n","        # Clear gradients w.r.t. parameters\n","        optimizer.zero_grad()\n","\n","        # Forward pass to get output/logits\n","        outputs = model(images)\n","\n","        # Calculate Loss: softmax --> cross entropy loss\n","        loss = criterion(outputs, labels)\n","\n","        # Getting gradients w.r.t. parameters\n","        loss.backward()\n","\n","        # Updating parameters\n","        optimizer.step()\n","\n","        iter += 1\n","\n","        if iter % 500 == 0:\n","            # Calculate Accuracy\n","            correct = 0\n","            total = 0\n","            # Iterate through test dataset\n","            for images, labels in test_loader:\n","                #######################\n","                #  USE GPU FOR MODEL  #\n","                #######################\n","                images = images.view(-1, 28*28).to(device)\n","\n","                # Forward pass only to get logits/output\n","                outputs = model(images)\n","\n","                # Get predictions from the maximum value\n","                _, predicted = torch.max(outputs.data, 1)\n","\n","                # Total number of labels\n","                total += labels.size(0)\n","\n","                #######################\n","                #  USE GPU FOR MODEL  #\n","                #######################\n","                # Total correct predictions\n","                if torch.cuda.is_available():\n","                    correct += (predicted.cpu() == labels.cpu()).sum()\n","                else:\n","                    correct += (predicted == labels).sum()\n","\n","            accuracy = 100 * correct.item() / total\n","\n","            # Print Loss\n","            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"]},{"cell_type":"markdown","metadata":{"id":"5lBX20DmWWD3"},"source":["> **Alternative Term of Neural Network**\n",">\n","> The alternative term is **Universal Function Approximator**. This is because ultimately we are trying to find a function that maps our input, $X$, to our output, $y$."]},{"cell_type":"markdown","metadata":{"id":"r7ehBqQXWWD3"},"source":["## Summary\n","\n","- **Linear Classifier Problems**\n","    - Cannot represent **non-linear** functions\n","        - $ y = 4x_1 + 2x_2^2 +3x_3^3 $\n","        - $ y = x_1x_2$\n","- Introduced **Non-Linearity** to Logistic Regression to form a Neural Network\n","- **Types** of Non-Linearity\n","    - Sigmoid\n","    - Tanh\n","    - ReLU\n","- Feedforward Neural Network **Models**\n","    - Model A: 1 hidden layer (**sigmoid** activation)\n","    - Model B: 1 hidden layer (**tanh** activation)\n","    - Model C: 1 hidden layer (**ReLU** activation)\n","    - Model D: **2 hidden** layers (ReLU activation)\n","    - Model E: **3 hidden** layers (ReLU activation)\n","- Models Variation in **Code**\n","    - Modifying only step 3\n","- Ways to Expand Model’s **Capacity**\n","    - More non-linear activation units (**neurons**)\n","    - More hidden **layers**\n","- **Cons** of Expanding Capacity\n","    - Need more **data**\n","    - Does not necessarily mean higher **accuracy**\n","- **GPU** Code\n","    - 2 things on GPU\n","        - **model**\n","        - **tensors with gradient accumulation capabilities**\n","    - Modifying only **Step 4 & Step 7**\n","- **7 Step** Model Building Recap\n","    - Step 1: Load Dataset\n","    - Step 2: Make Dataset Iterable\n","    - Step 3: Create Model Class\n","    - Step 4: Instantiate Model Class\n","    - Step 5: Instantiate Loss Class\n","    - Step 6: Instantiate Optimizer Class\n","    - Step 7: Train Model\n"]},{"cell_type":"markdown","metadata":{"id":"BfjPn70vWWD4"},"source":["### *References*\n","[1] [DOI](https://zenodo.org/badge/139945544.svg)(https://zenodo.org/badge/latestdoi/139945544)"]}],"metadata":{"colab":{"collapsed_sections":["vich-tIlWWDI","oNYiYAFPWWD0"],"provenance":[]},"kernelspec":{"display_name":"Python [default]","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":0}
